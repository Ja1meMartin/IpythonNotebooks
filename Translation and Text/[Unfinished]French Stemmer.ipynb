{
 "metadata": {
  "name": "",
  "signature": "sha256:0a9f48b59b259e9e8c4baa1f8b33510a873251d7f02a8b9bd7945b3a63bf9559"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get NLTK and PyPDF2\n",
      "from __future__ import print_function\n",
      "from nltk.stem import *\n",
      "import PyPDF2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "pdf = PyPDF2.PdfFileReader(open(\"hp.pdf\", \"rb\"))\n",
      "\n",
      "pages = [page for page in pdf.pages if len(page.extractText()) > 200]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Split into sentences  \n",
      "For now just split on full stops. To find all sentences create a collection of punctuation marks that end sentences (What if quotation marks come after punctuation mark?).  \n",
      "  \n",
      "Get the indices of all those marks:  \n",
      "  \n",
      "     sentence_ends = ['.','?','!']  \n",
      "     indices = [index for index, char in enumerate(text)]  \n",
      "    \n",
      "Now say your list is [77,255,382] you split from 0->77+1, 78->255+1, 382->len(text)\n",
      "\n",
      "_Need to figure out what to do with chapter and sentence numbers_"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = pages[0]\n",
      "sentences = t.extractText().split(\".\")\n",
      "print (sentences[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5  1 La maison des \u00ab Jeux du sort \u00bb Les habitants de Little Hangleton maison des \u00ab Jeux du sort ann\u00e9es que lfen\u00eatres condamn\u00e9es par des planches, le toit d\u00e9pourvu de qui poussait en toute libert\u00e9\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from unidecode import unidecode\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "stemmer = SnowballStemmer(\"french\")\n",
      "words = set()\n",
      "\n",
      "# How many words are in the set after each sentence is processed\n",
      "for sentence in sentences:\n",
      "    w = [w for w in sentence.split() if w.isalpha()]\n",
      "    words.update(w)\n",
      "    print (len(words))\n",
      "    \n",
      "# sep overrides the space that was in Python 2.7 print statement \n",
      "# not sure why I was decoding the words\n",
      "for word in words:\n",
      "    print (word, stemmer.stem(word), sep=\"---\")\n",
      "    try:\n",
      "        word.decode()\n",
      "        print('b')\n",
      "    except:\n",
      "        pass\n",
      "        #print (word, type(word[-1]))\n",
      "        #print(unidecode(word))\n",
      "    \n",
      "    \n",
      "\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "24\n",
        "35\n",
        "38\n",
        "72\n",
        "86\n",
        "lfen\u00eatres---lfen\u00eatr\n",
        "salon---salon\n",
        "b\n",
        "froid---froid\n",
        "b\n",
        "comme---comm\n",
        "b\n",
        "commen\u00e7aient---commenc\n",
        "famille---famill\n",
        "b\n",
        "ronde---rond\n",
        "b\n",
        "le---le\n",
        "b\n",
        "la---la\n",
        "b\n",
        "mani\u00e8re---mani\n",
        "libert\u00e9---libert\n",
        "cadavres---cadavr\n",
        "b\n",
        "de---de\n",
        "b\n",
        "imposante---impos\n",
        "b\n",
        "racont\u00e9e---racont\n",
        "sont---sont\n",
        "b\n",
        "condamn\u00e9es---condamn\n",
        "du---du\n",
        "b\n",
        "d\u00eener---d\u00een\n",
        "yeux---yeux\n",
        "b\n",
        "\u00e9v\u00e9nement---\u00e9ven\n",
        "Jeux---jeux\n",
        "b\n",
        "ouverts---ouvert\n",
        "b\n",
        "Little---littl\n",
        "b\n",
        "en---en\n",
        "b\n",
        "ann\u00e9es---ann\u00e9\n",
        "eu---eu\n",
        "b\n",
        "et---et\n",
        "b\n",
        "kilom\u00e8tres---kilometr\n",
        "toute---tout\n",
        "b\n",
        "Les---le\n",
        "b\n",
        "trouv\u00e9---trouv\n",
        "soigneusement---soigneux\n",
        "b\n",
        "Froids---froid\n",
        "b\n",
        "les---le\n",
        "b\n",
        "que---que\n",
        "b\n",
        "\u00e9tait---\u00e9tait\n",
        "qui---qui\n",
        "b\n",
        "m\u00eame---m\u00eam\n",
        "\u00e0---\u00e0\n",
        "Un---un\n",
        "b\n",
        "plus---plus\n",
        "b\n",
        "une---une\n",
        "b\n",
        "\u00e9difice---\u00e9dific\n",
        "\u00e9t\u00e9---\u00e9t\u00e9\n",
        "allong\u00e9s---allong\n",
        "entr\u00e9e---entr\u00e9\n",
        "avait---avait\n",
        "b\n",
        "toutes---tout\n",
        "b\n",
        "d\u00e9serte---d\u00e9sert\n",
        "enjoliv\u00e9e---enjoliv\n",
        "glace---glac\n",
        "b\n",
        "sort---sort\n",
        "b\n",
        "Jedusor---jedusor\n",
        "b\n",
        "avaavait---avaav\n",
        "b\n",
        "servante---serv\n",
        "b\n",
        "versions---version\n",
        "b\n",
        "des---de\n",
        "b\n",
        "dans---dan\n",
        "b\n",
        "pour---pour\n",
        "b\n",
        "un---un\n",
        "b\n",
        "tant---tant\n",
        "b\n",
        "encore---encor\n",
        "b\n",
        "La---la\n",
        "b\n",
        "habitants---habit\n",
        "b\n",
        "habill\u00e9s---habill\n",
        "r\u00e9sidence---r\u00e9sident\n",
        "par---par\n",
        "b\n",
        "r\u00e9cit---rec\n",
        "belle---bel\n",
        "b\n",
        "maison---maison\n",
        "b\n",
        "grand---grand\n",
        "b\n",
        "terre---terr\n",
        "b\n",
        "Hangleton---hangleton\n",
        "b\n",
        "Ils---il\n",
        "b\n",
        "poussait---pouss\n",
        "b\n",
        "Encore---encor\n",
        "b\n",
        "toit---toit\n",
        "b\n",
        "grands---grand\n",
        "b\n",
        "trois---trois\n",
        "b\n",
        "majestueux---majestu\n",
        "b\n",
        "d\u00e9pourvu---d\u00e9pourvu\n",
        "si---si\n",
        "b\n",
        "y---y\n",
        "b\n",
        "dos---dos\n",
        "b\n",
        "manoir---manoir\n",
        "b\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Stanford French POStagger](http://nlp.stanford.edu/software/tagger.shtml)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag.stanford import POSTagger\n",
      "\n",
      "import os\n",
      "java_path = \"C:\\Program Files (x86)\\Java\\jdk1.7.0_51/bin/java.exe\"\n",
      "os.environ['JAVAHOME'] = java_path\n",
      "\n",
      "st = POSTagger('c:\\Stanford\\models\\\\french.tagger','C:\\Stanford\\stanford-postagger.jar')\n",
      "\n",
      "tags = st.tag([unidecode(word) for word in words])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "lfenetres:  \n",
      "1. Why is the program ignoring apostrophes\n",
      "2. It does not give gender which hints at not using base word list or is it an oversight"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for t in tags:\n",
      "    print(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'lfenetres', u'ADJ')\n",
        "(u'salon', u'NC')\n",
        "(u'froid', u'ADJ')\n",
        "(u'comme', u'P')\n",
        "(u'commencaient', u'V')\n",
        "(u'famille', u'N')\n",
        "(u'ronde', u'ADJ')\n",
        "(u'le', u'DET')\n",
        "(u'la', u'DET')\n",
        "(u'maniere', u'NC')\n",
        "(u'liberte', u'ADJ')\n",
        "(u'cadavres', u'ADJ')\n",
        "(u'de', u'P')\n",
        "(u'imposante', u'NC')\n",
        "(u'racontee', u'ADJ')\n",
        "(u'sont', u'V')\n",
        "(u'condamnees', u'ADJ')\n",
        "(u'du', u'P')\n",
        "(u'diner', u'VINF')\n",
        "(u'yeux', u'NC')\n",
        "(u'evenement', u'ADV')\n",
        "(u'Jeux', u'N')\n",
        "(u'ouverts', u'ADJ')\n",
        "(u'Little', u'NPP')\n",
        "(u'en', u'P')\n",
        "(u'annees', u'NC')\n",
        "(u'eu', u'VPP')\n",
        "(u'et', u'CC')\n",
        "(u'kilometres', u'VPP')\n",
        "(u'toute', u'DET')\n",
        "(u'Les', u'DET')\n",
        "(u'trouve', u'V')\n",
        "(u'soigneusement', u'ADV')\n",
        "(u'Froids', u'NPP')\n",
        "(u'les', u'DET')\n",
        "(u'que', u'ADV')\n",
        "(u'etait', u'V')\n",
        "(u'qui', u'PROREL')\n",
        "(u'meme', u'CLO')\n",
        "(u'a', u'V')\n",
        "(u'Un', u'DET')\n",
        "(u'plus', u'ADV')\n",
        "(u'une', u'DET')\n",
        "(u'edifice', u'NC')\n",
        "(u'ete', u'ADJ')\n",
        "(u'allonges', u'ADJ')\n",
        "(u'entree', u'NC')\n",
        "(u'avait', u'V')\n",
        "(u'toutes', u'ADJ')\n",
        "(u'deserte', u'ADV')\n",
        "(u'enjolivee', u'ADJ')\n",
        "(u'glace', u'NC')\n",
        "(u'sort', u'V')\n",
        "(u'Jedusor', u'NPP')\n",
        "(u'avaavait', u'V')\n",
        "(u'servante', u'ADJ')\n",
        "(u'versions', u'NC')\n",
        "(u'des', u'P')\n",
        "(u'dans', u'P')\n",
        "(u'pour', u'P')\n",
        "(u'un', u'DET')\n",
        "(u'tant', u'ADV')\n",
        "(u'encore', u'ADV')\n",
        "(u'La', u'DET')\n",
        "(u'habitants', u'NC')\n",
        "(u'habilles', u'ADJ')\n",
        "(u'residence', u'V')\n",
        "(u'par', u'P')\n",
        "(u'recit', u'N')\n",
        "(u'belle', u'ADJ')\n",
        "(u'maison', u'N')\n",
        "(u'grand', u'ADJ')\n",
        "(u'terre', u'N')\n",
        "(u'Hangleton', u'N')\n",
        "(u'Ils', u'CLS')\n",
        "(u'poussait', u'V')\n",
        "(u'Encore', u'ADV')\n",
        "(u'toit', u'NC')\n",
        "(u'grands', u'ADJ')\n",
        "(u'trois', u'ADJ')\n",
        "(u'majestueux', u'ADJ')\n",
        "(u'depourvu', u'VPP')\n",
        "(u'si', u'CS')\n",
        "(u'y', u'CLO')\n",
        "(u'dos', u'NC')\n",
        "(u'manoir', u'NC')\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### From here we switch to Noun extraction\n",
      "\n",
      "\n",
      "The functions extract_entities and visible are based on code found on SO  \n",
      "  \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import feedparser\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "\n",
      "feed = 'http://arsenalyouth.wordpress.com/feed/'\n",
      "d = feedparser.parse(feed)\n",
      "links = [e.link for e in d['entries']]\n",
      "\n",
      "print (links[0])\n",
      "\n",
      "import nltk\n",
      "\n",
      "def getSoup(link):\n",
      "    url = link\n",
      "    content = url.read()\n",
      "    soup = BeautifulSoup(content)\n",
      "    text = soup.text\n",
      "    \n",
      "    print (\"text\", type(text))\n",
      "    \n",
      "    ps = soup.findAll(\"p\")\n",
      "    \n",
      "    def extract_entities(text):\n",
      "        for sent in nltk.sent_tokenize(text):\n",
      "            for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
      "                print (chunk)\n",
      "                    \n",
      "    # find capitalised words\n",
      "    # retext = [t for t in text.split() if t[0].isupper() and all([x.islower() for x in t[1:]]) and t.isalpha()]\n",
      "    \n",
      "    texts = soup.findAll(text=True)\n",
      "\n",
      "    def visible(element):\n",
      "        import re\n",
      "        if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
      "            return False\n",
      "        elif re.match('<!--.*-->', unicode(element)):\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    #visible_texts = filter(visible, texts)\n",
      "                    \n",
      "    #for l in text.split(\"\\n\"):\n",
      "    #extract_entities(\" \".join(visible_texts))\n",
      "        \n",
      "    \n",
      "\n",
      "    #from nltk.tag import pos_tag\n",
      "    #tagged = pos_tag([t for t in text.split() if t[0].isupper() and all([x.islower() for x in t[1:]]) and t.isalpha()])\n",
      "\n",
      "    #propernouns = [word for word,pos in tagged if pos == 'NNP']\n",
      "    \n",
      "    return (text, ps)#, propernouns)\n",
      "\n",
      "data = getSoup(links[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "https://arsenalyouth.wordpress.com/2015/01/13/arsenals-iwobi-receives-nigeria-call-up/\n",
        "text"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " <type 'unicode'>\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample = data[0]\n",
      "print (type(sample)) # getting unicode\n",
      "\n",
      "sentences = nltk.sent_tokenize(sample)\n",
      "#print (sentences)\n",
      "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
      "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
      "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
      " \n",
      "def extract_entity_names(t):\n",
      "    entity_names = []\n",
      "    if hasattr(t, 'node') and t.node:\n",
      "        if t.node == 'NE':\n",
      "            entity_names.append(' '.join([child[0] for child in t]))\n",
      "        else:\n",
      "            for child in t:\n",
      "                entity_names.extend(extract_entity_names(child))\n",
      "    return entity_names\n",
      " \n",
      "entity_names = []\n",
      "for tree in chunked_sentences:\n",
      "# Print results per sentence\n",
      "# print extract_entity_names(tree)\n",
      "    entity_names.extend(extract_entity_names(tree))\n",
      " \n",
      "# Print all entity names\n",
      "print (entity_names)\n",
      " \n",
      "# Print unique entity names\n",
      "print (set(entity_names))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'unicode'>\n",
        "[]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "set([])\n"
       ]
      }
     ],
     "prompt_number": 23
    }
   ],
   "metadata": {}
  }
 ]
}