{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division # means division can return decimals\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_all_punctuation_and_numbers(word):\n",
    "    '''\n",
    "        Retain only the letters in the word\n",
    "    '''\n",
    "    return \"\".join([c for c in word if c.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read = file.read \n",
    "\n",
    "\"\"\"\n",
    "    Read all of 'stop-word-list.csv'\n",
    "    Values are comma seperated so split on ','\n",
    "    Strip the whitespace round each word\n",
    "    Probably could have used lstrip but the gain\n",
    "    in efficiency is not worth the odds that there\n",
    "    was a space between a word and a commma\n",
    "\"\"\"\n",
    "with open(\"stop-word-list.csv\") as stopwords_file:\n",
    "        stopwords = [word.strip() for word in read(stopwords_file).split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple # much more efficient than objects\n",
    "from collections import defaultdict # a dict where you can set a default value\n",
    "from collections import Counter # creates automatically key:frequency dictionary\n",
    "import math\n",
    "\n",
    "class VectorSpaceModel(object):\n",
    "    \n",
    "    Document = namedtuple('Document',['title',  'term_frequencies'])\n",
    "    \n",
    "    def __init__(self, stopwords = [], **kwargs):\n",
    "        self.remove_punctuation = kwargs.get('punctuation', remove_all_punctuation_and_numbers) \n",
    "        self.stopwords = stopwords\n",
    "      \n",
    "        # dictionary to store how many documents a word was in\n",
    "        self.inverse_document_counts = defaultdict(lambda: 0)   \n",
    "\n",
    "        # downloaded the snowball stemming dictionary\n",
    "        # created a python dictionary and pickled it\n",
    "        self.stemming_dict = kwargs.get('stemming_dict', pickle.load( open( \"snowball.p\", \"rb\" ) ) )\n",
    "        \n",
    "        self.documents = [] # Holds named tuples\n",
    "        self.number_of_documents = 0\n",
    "        self.idf_squared = {} # key = document frequency : value = idf**2 to speed up calculations\n",
    "    \n",
    "    \n",
    "    def stem(self, word):\n",
    "        \"\"\"\n",
    "            Remove whitespace from the word and change to lower case for comparison\n",
    "            Check if it is in stopwords:\n",
    "            if it is look it up in the stemming dictionary\n",
    "            if it is not return None\n",
    "        \"\"\"\n",
    "        word = self.remove_punctuation(word.strip().lower()) # remove punctuation and change to lower case\n",
    "        if word not in self.stopwords:\n",
    "            return self.stemming_dict.get(word, word) # find a stem else return the word as is\n",
    "        else:\n",
    "            return None # this is a stopword so we ignore it\n",
    "\n",
    "    \n",
    "    def create_document(self, title, contents):\n",
    "        \"\"\"\n",
    "            Find all the words in this document\n",
    "            document_term_frequencies is how many times a word appears divided by the most\n",
    "            frequent word in the document. These are are used along with title to create\n",
    "            a named tuple for this document\n",
    "            inverse_document_count refers to how many documents a term appears in. A document\n",
    "            does not have knowledge of the collection it is in so can only incrememt the count.\n",
    "        \"\"\"\n",
    "        title=title.strip()\n",
    "        document_term_frequencies = {}\n",
    "        \n",
    "        from itertools import ifilter\n",
    "\n",
    "        # stem returns None for stopwords so we must remove them\n",
    "        contents = ifilter(None,(self.stem(word) for word in contents.split())) \n",
    "\n",
    "        counter = Counter(contents)\n",
    "\n",
    "        most_frequent_keyword = counter.most_common(1)[0][1]\n",
    "\n",
    "        for kw in counter.keys():\n",
    "\n",
    "            document_term_frequencies[kw] = counter[kw]/most_frequent_keyword # term frequency\n",
    "            self.inverse_document_counts[kw] += 1 # inverse document count\n",
    "\n",
    "\n",
    "        doc_tuple = VectorSpaceModel.Document(title = title,  term_frequencies = document_term_frequencies)\n",
    "\n",
    "        self.documents.append( doc_tuple )\n",
    "            \n",
    "         \n",
    "    def tf_idf(self):\n",
    "\n",
    "        tf_idf = defaultdict(lambda: [])\n",
    "        set_of_document_counts = set(self.inverse_document_counts.values()) \n",
    "        self.number_of_documents = len(self.documents)\n",
    "\n",
    "        # cache the values of idf^2 for a given document frequency\n",
    "        # assumes document idf = query idf\n",
    "        self.idf_squared = {count : (self.inverse_document_frequency_method(count)**2) for count in set_of_document_counts}\n",
    "        self.idf_squared.update({0:0})\n",
    "        # Get the lengths of vectors\n",
    "        self.document_lengths = {document.title: self.get_vector_length(document) for document in self.documents}\n",
    "     \n",
    "   \n",
    "    def get_vector_length(self,document):\n",
    "        \n",
    "        \"\"\"\n",
    "            Vector length is math.sqrt(sum(tf_idf**2))\n",
    "            tf_idf**2 = tf*idf*tf*idf = tf**2 * idf**2\n",
    "            \n",
    "            idf_squared already calculated\n",
    "            tf is frequency divide by most frequent word\n",
    "            \n",
    "        \"\"\"\n",
    "        tf_dict = document.term_frequencies\n",
    "        idf_squared = self.idf_squared\n",
    "        document_counts = self.inverse_document_counts\n",
    "        \n",
    "        \n",
    "        query_length_squared = sum(idf_squared[document_counts [kw]] * (tf**2) for kw, tf in tf_dict.iteritems())\n",
    "            \n",
    "        vector_length = math.sqrt(query_length_squared)\n",
    "        \n",
    "        return vector_length\n",
    "    \n",
    "    def query(self, string):\n",
    "        \n",
    "        from itertools import ifilter\n",
    "        \n",
    "        keywords = ifilter (None, (self.stem(word) for word in string.split())) # Stemming\n",
    "        \n",
    "        counter = Counter(keywords) # Count frequencies\n",
    "        \n",
    "        most_frequent = counter.most_common(1)[0][1] # returns list of tuples hence long access line\n",
    "        query_tf_dict = {key: value/most_frequent for key, value in counter.iteritems()}\n",
    "        \n",
    "        # get_vector_length expects a document so turn query into document\n",
    "        query = self.Document(title=\"query\",  term_frequencies=query_tf_dict)\n",
    "        query_length = self.get_vector_length(query)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "            Dot product:\n",
    "            \n",
    "            sum of\n",
    "            for key value in tf_dict:\n",
    "                \n",
    "                (query tf * term idf) * (document tf * term idf)\n",
    "                = query tf * document tf * term idf squared \n",
    "        \"\"\"\n",
    "        \n",
    "        document_scores = []\n",
    "        titles = [document.title for document in self.documents]\n",
    "        \n",
    "        for document in self.documents:    \n",
    "                                                                    \n",
    "            dot_product = sum(document.term_frequencies.get(key, 0) * query_tf * \n",
    "                                      self.idf_squared[self.inverse_document_counts[key]] \\\n",
    "                                      for key, query_tf in query_tf_dict.iteritems())\n",
    "      \n",
    "            \n",
    "            document_length = self.document_lengths[document.title]\n",
    "            \n",
    "            cos = dot_product/(document_length * query_length)\n",
    "            \n",
    "            document_scores.append(cos)\n",
    "            \n",
    "        return sorted(zip(titles, document_scores), key = lambda x: x[1], reverse = True)[:10]\n",
    "    \n",
    "    \n",
    "    def inverse_document_frequency_method(self, doc_frequency):\n",
    "        if doc_frequency:\n",
    "            return math.log10(self.number_of_documents/doc_frequency)\n",
    "        else: return 0 # to avoid divison by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VSM = VectorSpaceModel(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "for page in pickle.load(open('vpl.p', 'r')):\n",
    "    with open(page) as f:\n",
    "        source = f.read()\n",
    "        soup = BeautifulSoup(source)\n",
    "    \n",
    "        # http://stackoverflow.com/questions/5598524/can-i-remove-script-tags-with-beautifulsoup\n",
    "        [script.extract() for script in soup('script')]\n",
    "        # Removing comments appears to be btoken in Beautiful Soup so use regular expression instead\n",
    "        # http://stackoverflow.com/questions/28208186/how-remove-html-comments-using-regex-in-python\n",
    "        # Can't figure out how to remove comments within comments\n",
    "        # Could use re.subn which returns a tuple (string, number_of_replacements_made)\n",
    "        # until number_of_replacements_made = 0\n",
    "        source = re.sub(\"(<!--.*?-->)\", \"\", soup.get_text(), flags=re.MULTILINE)\n",
    "            \n",
    "        VSM.create_document(page, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VSM.tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Curiouser said Alice\",\n",
    "    \"late white rabbit\",\n",
    "    \"Snark hunting is fun\",\n",
    "    \"Vorpal blade\",\n",
    "    \"Snicker Carpenter Snack Oyster\",\n",
    "    \"Professor Gardener Waggly Revenge\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    filename = \"Queries/\" + query + \".txt\"\n",
    "    results = VSM.query(query)\n",
    "    with open(filename, \"w\") as f:\n",
    "        for page, value in results:\n",
    "            line = \" --> \" . join ([page, str(value)])\n",
    "            f.write(line + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a query (a blank query will exit the program).\n",
      "Alice\n",
      "literature.offline/authors/carroll-lewis/through-the-looking-glass/chapter-02.html 0.403044578408\n",
      "literature.offline/authors/carroll-lewis/through-the-looking-glass/chapter-05.html 0.368241212534\n",
      "literature.offline/authors/carroll-lewis/through-the-looking-glass/chapter-09.html 0.366914084489\n",
      "literature.offline/authors/carroll-lewis/alices-adventures-in-wonderland/chapter-08.html 0.358736909461\n",
      "literature.offline/authors/carroll-lewis/alices-adventures-in-wonderland/chapter-06.html 0.351277210862\n",
      "literature.offline/authors/carroll-lewis/through-the-looking-glass/chapter-03.html 0.349364379233\n",
      "literature.offline/authors/carroll-lewis/alices-adventures-in-wonderland/chapter-01.html 0.349077170615\n",
      "literature.offline/authors/carroll-lewis/alices-adventures-in-wonderland/chapter-09.html 0.315926586011\n",
      "literature.offline/authors/carroll-lewis/alices-adventures-in-wonderland/chapter-05.html 0.292252467386\n",
      "literature.offline/authors/carroll-lewis/alices-adventures-in-wonderland/chapter-04.html 0.287834697758\n",
      "\n",
      "Please enter a query (a blank query will exit the program).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print \"Please enter a query (a blank query will exit the program).\"\n",
    "    input = raw_input()\n",
    "    \n",
    "    if not input:\n",
    "        break\n",
    "        \n",
    "    for page,value in VSM.query(input):\n",
    "        print page, value\n",
    "        \n",
    "    print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
