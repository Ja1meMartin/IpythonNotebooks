{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_all_punctuation_and_numbers(word):\n",
    "    '''\n",
    "        If the last digit is not alpha then remove it\n",
    "    '''\n",
    "\n",
    "    if not word[-1].isalpha():\n",
    "        word = word[:-1]\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division # means division can return decimals\n",
    "from collections import defaultdict # a dict where you can set a default value\n",
    "from collections import namedtuple # much more efficient than objects\n",
    "from collections import Counter # creates automatically key:frequency dictionary\n",
    "import math\n",
    "\n",
    "corpus = \"\"\"\n",
    "d1: for English model retrieval have a relevance model while vector space\n",
    "model retrieval do not;\n",
    "d2: The R-precision measure is relevant to average precision measure.;\n",
    "d3: The most efficient retrieval models are language model and vector space\n",
    "model.;\n",
    "d4: The English language is the most efficient language.;\n",
    "d5: Retrieval efficiency is measured by the average precision of the\n",
    "retrieval model.\n",
    "\"\"\"\n",
    "\n",
    "stems = {\n",
    "    \"models\":\"model\",\n",
    "    \"r-precision\":\"precis\",\n",
    "    \"precision\":\"precis\",\n",
    "    \"precise\":\"precis\",\n",
    "    \"efficient\":\"effic\",\n",
    "    \"efficiency\":\"effic\",\n",
    "    \"recall\":\"retrieval\",\n",
    "    \"relevant\":\"relevan\",\n",
    "    \"relevance\":\"relevan\",\n",
    "    \"measured\":\"measure\",\n",
    "}\n",
    "\n",
    "string = \"\"\"a , at, are, for, of, I , is, there, then, many, do, to, and, by, the, not, have, with, while\"\"\"\n",
    "\n",
    "stop_words = [s.strip() for s in string.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1 - KEYWORDS: ['space', 'vector', 'english', 'relevan', 'model', 'retrieval']\n",
      "d2 - KEYWORDS: ['relevan', 'precis', 'precis', 'measure', 'measure', 'average']\n",
      "d3 - KEYWORDS: ['model', 'language', 'space', 'effic', 'most', 'vector', 'model', 'retrieval', 'model']\n",
      "d4 - KEYWORDS: ['language', 'effic', 'language', 'most', 'english']\n",
      "d5 - KEYWORDS: ['precis', 'effic', 'model', 'average', 'retrieval', 'measure', 'retrieval']\n",
      "{'language': [0, 0, 0.6, 0.6, 0], 'space': [0.4, 0, 0.4, 0, 0], 'average': [0, 0.4, 0, 0, 0.4], 'measure': [0, 0.6, 0, 0, 0.6], 'precis': [0, 0.6, 0, 0, 0.6], 'most': [0, 0, 0.4, 0.4, 0], 'relevan': [0.4, 0.4, 0, 0, 0], 'vector': [0.4, 0, 0.4, 0, 0], 'english': [0.4, 0, 0, 0.4, 0], 'model': [1.0, 0, 1.0, 0, 1.0], 'effic': [0, 0, 0.6, 0.6, 0.6], 'retrieval': [0.8, 0, 0.8, 0, 0.8]}\n"
     ]
    }
   ],
   "source": [
    "class ExtendedBooleanModel(object):\n",
    "    \"\"\"\n",
    "        Preprocessing:\n",
    "        1. Gathering\n",
    "        2. Stemming\n",
    "        3. Stopword Removal\n",
    "        4. Indexing\n",
    "    \n",
    "        1a. Read in documents use doc_sep to seperate docs (default \";\") done in constructor\n",
    "        1b. Seperate each doc into name and contentsusing title_sep (default = \":\") \n",
    "        1c. Break into constituent parts if TF-IDF we need to count occurrences\n",
    "        \n",
    "        2a. lowercase\n",
    "        2b. remove punctuation #NEED DEFAULT PUNCTUATION LIST\n",
    "        2c. use stemming dictionary (deafault = None) \n",
    "        ??  Check for hyphens\n",
    "        \n",
    "        3a. Remove stopwords (default in __stopwords__)\n",
    "\n",
    "        4a. Create index\n",
    "        \n",
    "        \n",
    "        inverse document frequency:\n",
    "        for each document\n",
    "        - \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        If we are going to have a lot of documents then the overhead of objet creation will slow us down\n",
    "        'title' is the name of the document\n",
    "        'keyword' is the set of stemmed words in the document \n",
    "    \"\"\"\n",
    "    Document = namedtuple('Document',['title', 'keywords'])\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, stopwords, **kwargs):\n",
    "        # kwargs.get(foo, default) searches for named arguments e.g pnorm=4 but provides a default if not found\n",
    "        self.pnorm = kwargs.get('pnorm',2)\n",
    "        self.doc_sep = kwargs.get('doc_sep', ';') # seperates documents in corpus\n",
    "        self.title_sep = kwargs.get('title_sep', ':') # seperates document title from comments\n",
    "        self.remove_punctuation = kwargs.get('punctuation', None) # function to remove punctuation\n",
    "        self.stopwords = stopwords\n",
    "        self.pnorm = kwargs.get(\"pnorm\", 2)\n",
    "        self.last_query = None \n",
    "        \n",
    "        \"\"\"\n",
    "            We set defaultdict default to 0. Now we can go through each documents keywords and call:\n",
    "                self.term_weights += 1\n",
    "            We do not have to worry wether the word was in the dictionary already.\n",
    "            This is performed as a side effect in create_document_tuples to avoid multiple passes\n",
    "            An alternative to explore: join the sets in a list and pass them to a Counter object\n",
    "        \"\"\"\n",
    "        self.term_weights = defaultdict(lambda: 0)\n",
    "        \n",
    "        # default function to remove punctuation\n",
    "        if not self.remove_punctuation:\n",
    "            self.remove_punctuation = remove_all_punctuation_and_numbers\n",
    "            \n",
    "        self.stemming_dict = kwargs.get('stemming_dict', None)\n",
    "        \n",
    "        # Needs to be split into doc and contents\n",
    "        self._preprocess(corpus)\n",
    "        \n",
    "    def _preprocess(self, corpus):\n",
    "        \n",
    "        self.documents = self._read_documents(corpus) # creates the self.documents list of namedtuples\n",
    "        self.number_of_documents = len(self.documents) # save vale for calcs, if need to add docs may need to change\n",
    "        \n",
    "        # in _read_documents::create_document_tuples frequencies of terms\n",
    "        # were added to self.term weights. Divide this by the number of docs\n",
    "        # An alternative: add 1/len(docs) but hardly seems worth it and potential loss of accuracy\n",
    "        self.term_weights = {key: value/self.number_of_documents for key, value in self.term_weights.items()}\n",
    "        \n",
    "        self.index = self.build_index() # builds a dictionary of word:list pairs\n",
    "        print self.index\n",
    "    \n",
    "    def _read_documents(self, corpus):\n",
    "        \"\"\"\n",
    "            self.corpus.split(self.doc_sep) creates individual document strings (1a)\n",
    "            Each document performs:\n",
    "                - speration of document string into title and contents using self.title_sep\n",
    "                - transition to lower case\n",
    "                - removal of punctuation\n",
    "                - stemming\n",
    "                - stopword removal\n",
    "        \"\"\"\n",
    "       \n",
    "        return [self.create_document_tuples(self, doc) for doc in corpus.split(self.doc_sep) if doc]\n",
    "        \n",
    "    def stem(self, word):\n",
    "        word = self.remove_punctuation(word.strip().lower()) \n",
    "        \n",
    "        if word not in self.stopwords:\n",
    "            return self.stemming_dict.get(word, word)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "         \n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"\n",
    "            Builds a dictionary of word:list pairs\n",
    "            For each word,value create a list\n",
    "                For each document append value, if word is in document.keywords, or else 0 to the list\n",
    "                \n",
    "            So corpus frequency for a word is 0.4 and word appears in docs 1 and 5 should return [0.4,0,0,0,0.4]\n",
    "        \"\"\"\n",
    "        index = {}\n",
    "\n",
    "        for word, value in self.term_weights.items(): # value = corpus frequency\n",
    "            vector = []\n",
    "\n",
    "            for d in self.documents:\n",
    "                vector.append(value) if word in d.keywords else vector.append(0)\n",
    "                index[word] = vector\n",
    "                \n",
    "        return index\n",
    "        \n",
    "            \n",
    "    def create_document_tuples(self, model, string):\n",
    "        \n",
    "        title, contents = string.split(self.title_sep)\n",
    "\n",
    "        title=title.strip()\n",
    "        \n",
    "        # term frequency irrelevant so use set to avoid stemming multiple words\n",
    "        candidate_keywords = set(contents.split()) \n",
    "        keywords = filter(None,(self.stem(word) for word in candidate_keywords))\n",
    "        \n",
    "        print title,  \"- KEYWORDS:\", keywords\n",
    "        \n",
    "        for kw in keywords:\n",
    "            self.term_weights[kw] += 1\n",
    "            \n",
    "            \n",
    "        doc_tuple = ExtendedBooleanModel.Document(title = title, keywords = keywords)\n",
    "\n",
    "        return doc_tuple\n",
    "    \n",
    "        \n",
    "    \n",
    "    def OR(self, *args):\n",
    "        \n",
    "        \"\"\"\n",
    "            Evaluates an OR query in the Extended Boolean Model\n",
    "            'args' is of variable length and will either be a word from the query or the list of weights associated\n",
    "            with that query i.e [w1, w2, ... wn] where weight wn is the corpus frequency for a word in document n\n",
    "            \n",
    "            Any words in args are converted to the appropriate list of weights using @apply_weights\n",
    "            \n",
    "            Now we have 'terms' which is a list of list where the nth entry of each list refers to document n. We can\n",
    "            join the nth entries with the zip command - using *, the splat operator, since we have a nested list - \n",
    "            see Zip and Splat below. \n",
    "            \n",
    "            This forms an unnamed intermediate data structure on which we call apply_pnorm on each and this converts\n",
    "            our lists into single entries which have to be raised to 1/pnorm before being returned.                    \n",
    "        \"\"\"\n",
    "        \n",
    "        def apply_weights(word):\n",
    "            \"\"\"\n",
    "                This takes a word from the query and stems it and looks up its weight in the index\n",
    "            \"\"\"\n",
    "            print \"In OR.apply_weight with term:\", word, \"-->\", self.index[self.stem(word)]\n",
    "            return self.index[self.stem(word)]\n",
    "            #return [item**pnorm for item in weights]\n",
    "            \n",
    "        def apply_pnorm(weights):\n",
    "            \"\"\"\n",
    "                Thia takes in a list of term weights that are raised to the power of pnorm\n",
    "                This is then summed and divided by length of the list\n",
    "                All this is raised to the power of 1/pnorm i.e the inverse root\n",
    "            \"\"\"\n",
    "            \n",
    "            print \"\\nIn OR.apply_pnorm with p-norm =\", pnorm\n",
    "            print weights, \"--> (1-w)^p\", [x**pnorm for x in weights], \"--> sum =\", sum(x**pnorm for x in weights), \n",
    "            print \"sum/n =\" , sum(x**pnorm for x in weights)/len(weights)\n",
    "            print \"returned value\", (sum(x**pnorm for x in weights)/len(weights))**(1/pnorm)\n",
    "        \n",
    "            return (sum(x**pnorm for x in weights)/len(weights))**(1/pnorm)\n",
    "        print \"OR function with arguments:\", \",\".join(str(a) for a in args )\n",
    "        \n",
    "        pnorm = self.pnorm \n",
    "\n",
    "        terms = (apply_weights(a) if not isinstance(a, list) else a for a in args)\n",
    "        weights = [apply_pnorm(x) for x in zip(*terms)]\n",
    "        \n",
    "        print \"weights after pnorm\", [round(w, 4) for w in weights]\n",
    "        print \"\\n=========================================\\n\"\n",
    "        return weights\n",
    "        \n",
    "        \n",
    "    def AND(self, *args):\n",
    "        \n",
    "        \"\"\"\n",
    "            Evaluates an AND query in the Extended Boolean Model\n",
    "            'args' is of variable length and will either be a word from the query or the list of weights associated\n",
    "            with that query i.e [w1, w2, ... wn] where weight wn is the corpus frequency for a word in document n\n",
    "            \n",
    "            Any words in args are converted to the appropriate list of weights using @apply_weights\n",
    "            \n",
    "            Now we have 'terms' which is a list of list where the nth entry of each list refers to document n. We can\n",
    "            join the nth entries with the zip command - using *, the splat operator, since we have a nested list - \n",
    "            see Zip and Splat below. \n",
    "            \n",
    "            This forms an unnamed intermediate data structure on which we call @apply_pnorm on each and this converts\n",
    "            our lists into single entries which have to be raised to 1/pnorm before being returned.                    \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        def apply_weights(word):\n",
    "            \"\"\"\n",
    "                This takes a word from the query and stems it and looks up its weight in the index\n",
    "            \"\"\"\n",
    "            print \"In AND.apply_weight with term:\", word, \"-->\", self.index[self.stem(word)]\n",
    "            return self.index[self.stem(word)]\n",
    "            \n",
    "        def apply_pnorm(weights):\n",
    "            \"\"\"\n",
    "                Thia takes in a list of term weights that are then subtracted from one\n",
    "                and the result is raised to the power of pnorm\n",
    "                This is then summed and divided by length of the list\n",
    "                All this is raised to the power of 1/pnorm i.e the inverse root\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            print \"\\nIn AND.apply_pnorm with p-norm =\", pnorm\n",
    "            print weights, \"--> (1-w)^p\", [(1-x)**pnorm for x in weights], \"--> sum =\", sum((1-x)**pnorm for x in weights), \n",
    "            print \"sum/n =\" , sum((1-x)**pnorm for x in weights)/len(weights), \"-->(sum/n)**1/p =\",\n",
    "            print (sum((1-x)**pnorm for x in weights)/len(weights))**(1/pnorm)\n",
    "            print \"returned value\", 1- (sum((1-x)**pnorm for x in weights)/len(weights))**(1/pnorm)\n",
    "            \n",
    "            \n",
    "            return 1- (sum((1-x)**pnorm for x in weights)/len(weights))**(1/pnorm)\n",
    "        \n",
    "        pnorm = self.pnorm\n",
    "        print \"AND function with arguments:\", \",\".join(str(a) for a in args)\n",
    "        \n",
    "        terms = [apply_weights(a) if not isinstance(a, list) else a for a in args ]\n",
    "        weights = [apply_pnorm(x) for x in zip(*terms)]\n",
    "        print \"\\n1 - weights after pnorm\", [round(w, 4) for w in weights]\n",
    "        print \"\\n=========================================\"\n",
    "        return weights\n",
    "                   \n",
    "        \n",
    "ebm = ExtendedBooleanModel(corpus, stop_words, stemming_dict=stems) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OR = ebm.OR\n",
    "AND = ebm.AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: relevant retrieval \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR function with arguments: relevant,retrieval\n",
      "In OR.apply_weight with term: relevant --> [0.4, 0.4, 0, 0, 0]\n",
      "In OR.apply_weight with term: retrieval --> [0.8, 0, 0.8, 0, 0.8]\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0.4, 0.8) --> (1-w)^p [0.16000000000000003, 0.6400000000000001] --> sum = 0.8 sum/n = 0.4\n",
      "returned value 0.632455532034\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0.4, 0) --> (1-w)^p [0.16000000000000003, 0] --> sum = 0.16 sum/n = 0.08\n",
      "returned value 0.282842712475\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0, 0.8) --> (1-w)^p [0, 0.6400000000000001] --> sum = 0.64 sum/n = 0.32\n",
      "returned value 0.565685424949\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0, 0) --> (1-w)^p [0, 0] --> sum = 0 sum/n = 0.0\n",
      "returned value 0.0\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0, 0.8) --> (1-w)^p [0, 0.6400000000000001] --> sum = 0.64 sum/n = 0.32\n",
      "returned value 0.565685424949\n",
      "weights after pnorm [0.6325, 0.2828, 0.5657, 0.0, 0.5657]\n",
      "\n",
      "=========================================\n",
      "\n",
      "d1 has a weight of 0.632455532034\n",
      "d3 has a weight of 0.565685424949\n",
      "d5 has a weight of 0.565685424949\n",
      "d2 has a weight of 0.282842712475\n",
      "d4 has a weight of 0.0\n"
     ]
    }
   ],
   "source": [
    "weights = OR('relevant', 'retrieval')\n",
    "\n",
    "for document, weight in sorted(zip(ebm.documents, weights), key= lambda x: x[1], reverse = True):\n",
    "    print document.title, \"has a weight of\", weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: efficient model efficient retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND function with arguments: efficient,model,efficient,retrieval\n",
      "In AND.apply_weight with term: efficient --> [0, 0, 0.6, 0.6, 0.6]\n",
      "In AND.apply_weight with term: model --> [1.0, 0, 1.0, 0, 1.0]\n",
      "In AND.apply_weight with term: efficient --> [0, 0, 0.6, 0.6, 0.6]\n",
      "In AND.apply_weight with term: retrieval --> [0.8, 0, 0.8, 0, 0.8]\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0, 1.0, 0, 0.8) --> (1-w)^p [1, 0.0, 1, 0.03999999999999998] --> sum = 2.04 sum/n = 0.51 -->(sum/n)**1/p = 0.714142842854\n",
      "returned value 0.285857157146\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0, 0, 0, 0) --> (1-w)^p [1, 1, 1, 1] --> sum = 4 sum/n = 1.0 -->(sum/n)**1/p = 1.0\n",
      "returned value 0.0\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0.6, 1.0, 0.6, 0.8) --> (1-w)^p [0.16000000000000003, 0.0, 0.16000000000000003, 0.03999999999999998] --> sum = 0.36 sum/n = 0.09 -->(sum/n)**1/p = 0.3\n",
      "returned value 0.7\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0.6, 0, 0.6, 0) --> (1-w)^p [0.16000000000000003, 1, 0.16000000000000003, 1] --> sum = 2.32 sum/n = 0.58 -->(sum/n)**1/p = 0.761577310586\n",
      "returned value 0.238422689414\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0.6, 1.0, 0.6, 0.8) --> (1-w)^p [0.16000000000000003, 0.0, 0.16000000000000003, 0.03999999999999998] --> sum = 0.36 sum/n = 0.09 -->(sum/n)**1/p = 0.3\n",
      "returned value 0.7\n",
      "\n",
      "1 - weights after pnorm [0.2859, 0.0, 0.7, 0.2384, 0.7]\n",
      "\n",
      "=========================================\n",
      "d3 has a weight of 0.7\n",
      "d5 has a weight of 0.7\n",
      "d1 has a weight of 0.285857157146\n",
      "d4 has a weight of 0.238422689414\n",
      "d2 has a weight of 0.0\n"
     ]
    }
   ],
   "source": [
    "weights = AND(\"efficient\", \"model\", \"efficient\", \"retrieval\")\n",
    "\n",
    "for document, weight in sorted(zip(ebm.documents, weights), key= lambda x: x[1], reverse = True):\n",
    "    print document.title, \"has a weight of\", weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: precise precision with average recall\n",
    "\n",
    "This will be represented by the following code:\n",
    "\n",
    "```python\n",
    "OR(AND(\"precise\",\"recall\"), \"average\")`\n",
    "```\n",
    "\n",
    "It will be processed by depth first search. If a function is reached it will continue down the tree until it reaches a node whose children are all words. As here the AND node has.\n",
    "<img src=\"https://chart.googleapis.com/chart?cht=gv:dot&amp;chl=digraph{ OR [style=filled,fillcolor=forestgreen]; AND[style=filled,fillcolor=red]; OR->AND[color=red] ;  AND->precise; AND->recall; OR->average}&amp;chs=250x250\" alt=\"neato chart\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND function with arguments: precise,recall\n",
      "In AND.apply_weight with term: precise --> [0, 0.6, 0, 0, 0.6]\n",
      "In AND.apply_weight with term: recall --> [0.8, 0, 0.8, 0, 0.8]\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0, 0.8) --> (1-w)^p [1, 0.03999999999999998] --> sum = 1.04 sum/n = 0.52 -->(sum/n)**1/p = 0.721110255093\n",
      "returned value 0.278889744907\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0.6, 0) --> (1-w)^p [0.16000000000000003, 1] --> sum = 1.16 sum/n = 0.58 -->(sum/n)**1/p = 0.761577310586\n",
      "returned value 0.238422689414\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0, 0.8) --> (1-w)^p [1, 0.03999999999999998] --> sum = 1.04 sum/n = 0.52 -->(sum/n)**1/p = 0.721110255093\n",
      "returned value 0.278889744907\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0, 0) --> (1-w)^p [1, 1] --> sum = 2 sum/n = 1.0 -->(sum/n)**1/p = 1.0\n",
      "returned value 0.0\n",
      "\n",
      "In AND.apply_pnorm with p-norm = 2\n",
      "(0.6, 0.8) --> (1-w)^p [0.16000000000000003, 0.03999999999999998] --> sum = 0.2 sum/n = 0.1 -->(sum/n)**1/p = 0.316227766017\n",
      "returned value 0.683772233983\n",
      "\n",
      "1 - weights after pnorm [0.2789, 0.2384, 0.2789, 0.0, 0.6838]\n",
      "\n",
      "=========================================\n",
      "OR function with arguments: [0.2788897449072021, 0.2384226894136091, 0.2788897449072021, 0.0, 0.683772233983162],average\n",
      "In OR.apply_weight with term: average --> [0, 0.4, 0, 0, 0.4]\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0.2788897449072021, 0) --> (1-w)^p [0.07777948981440425, 0] --> sum = 0.0777794898144 sum/n = 0.0388897449072\n",
      "returned value 0.197204829827\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0.2384226894136091, 0.4) --> (1-w)^p [0.05684537882721832, 0.16000000000000003] --> sum = 0.216845378827 sum/n = 0.108422689414\n",
      "returned value 0.329276007953\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0.2788897449072021, 0) --> (1-w)^p [0.07777948981440425, 0] --> sum = 0.0777794898144 sum/n = 0.0388897449072\n",
      "returned value 0.197204829827\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0.0, 0) --> (1-w)^p [0.0, 0] --> sum = 0.0 sum/n = 0.0\n",
      "returned value 0.0\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 2\n",
      "(0.683772233983162, 0.4) --> (1-w)^p [0.46754446796632404, 0.16000000000000003] --> sum = 0.627544467966 sum/n = 0.313772233983\n",
      "returned value 0.560153759233\n",
      "weights after pnorm [0.1972, 0.3293, 0.1972, 0.0, 0.5602]\n",
      "\n",
      "=========================================\n",
      "\n",
      "d5 has a weight of 0.560153759233\n",
      "d2 has a weight of 0.329276007953\n",
      "d1 has a weight of 0.197204829827\n",
      "d3 has a weight of 0.197204829827\n",
      "d4 has a weight of 0.0\n"
     ]
    }
   ],
   "source": [
    "weights = OR(AND(\"precise\",\"recall\"), \"average\")\n",
    "\n",
    "for document, weight in sorted(zip(ebm.documents, weights), key= lambda x: x[1], reverse = True):\n",
    "    print document.title, \"has a weight of\", weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate P-Norms for Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR function with arguments: relevant,retrieval\n",
      "In OR.apply_weight with term: relevant --> [0.4, 0.4, 0, 0, 0]\n",
      "In OR.apply_weight with term: retrieval --> [0.8, 0, 0.8, 0, 0.8]\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 1\n",
      "(0.4, 0.8) --> (1-w)^p [0.4, 0.8] --> sum = 1.2 sum/n = 0.6\n",
      "returned value 0.6\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 1\n",
      "(0.4, 0) --> (1-w)^p [0.4, 0] --> sum = 0.4 sum/n = 0.2\n",
      "returned value 0.2\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 1\n",
      "(0, 0.8) --> (1-w)^p [0, 0.8] --> sum = 0.8 sum/n = 0.4\n",
      "returned value 0.4\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 1\n",
      "(0, 0) --> (1-w)^p [0, 0] --> sum = 0 sum/n = 0.0\n",
      "returned value 0.0\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 1\n",
      "(0, 0.8) --> (1-w)^p [0, 0.8] --> sum = 0.8 sum/n = 0.4\n",
      "returned value 0.4\n",
      "weights after pnorm [0.6, 0.2, 0.4, 0.0, 0.4]\n",
      "\n",
      "=========================================\n",
      "\n",
      "OR function with arguments: relevant,retrieval\n",
      "In OR.apply_weight with term: relevant --> [0.4, 0.4, 0, 0, 0]\n",
      "In OR.apply_weight with term: retrieval --> [0.8, 0, 0.8, 0, 0.8]\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 10\n",
      "(0.4, 0.8) --> (1-w)^p [0.00010485760000000006, 0.10737418240000006] --> sum = 0.10747904 sum/n = 0.05373952\n",
      "returned value 0.746499254419\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 10\n",
      "(0.4, 0) --> (1-w)^p [0.00010485760000000006, 0] --> sum = 0.0001048576 sum/n = 5.24288e-05\n",
      "returned value 0.373213196615\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 10\n",
      "(0, 0.8) --> (1-w)^p [0, 0.10737418240000006] --> sum = 0.1073741824 sum/n = 0.0536870912\n",
      "returned value 0.746426393229\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 10\n",
      "(0, 0) --> (1-w)^p [0, 0] --> sum = 0 sum/n = 0.0\n",
      "returned value 0.0\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 10\n",
      "(0, 0.8) --> (1-w)^p [0, 0.10737418240000006] --> sum = 0.1073741824 sum/n = 0.0536870912\n",
      "returned value 0.746426393229\n",
      "weights after pnorm [0.7465, 0.3732, 0.7464, 0.0, 0.7464]\n",
      "\n",
      "=========================================\n",
      "\n",
      "OR function with arguments: relevant,retrieval\n",
      "In OR.apply_weight with term: relevant --> [0.4, 0.4, 0, 0, 0]\n",
      "In OR.apply_weight with term: retrieval --> [0.8, 0, 0.8, 0, 0.8]\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 100\n",
      "(0.4, 0.8) --> (1-w)^p [1.6069380442589993e-40, 2.0370359763344975e-10] --> sum = 2.03703597633e-10 sum/n = 1.01851798817e-10\n",
      "returned value 0.79447399635\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 100\n",
      "(0.4, 0) --> (1-w)^p [1.6069380442589993e-40, 0] --> sum = 1.60693804426e-40 sum/n = 8.03469022129e-41\n",
      "returned value 0.397236998175\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 100\n",
      "(0, 0.8) --> (1-w)^p [0, 2.0370359763344975e-10] --> sum = 2.03703597633e-10 sum/n = 1.01851798817e-10\n",
      "returned value 0.79447399635\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 100\n",
      "(0, 0) --> (1-w)^p [0, 0] --> sum = 0 sum/n = 0.0\n",
      "returned value 0.0\n",
      "\n",
      "In OR.apply_pnorm with p-norm = 100\n",
      "(0, 0.8) --> (1-w)^p [0, 2.0370359763344975e-10] --> sum = 2.03703597633e-10 sum/n = 1.01851798817e-10\n",
      "returned value 0.79447399635\n",
      "weights after pnorm [0.7945, 0.3972, 0.7945, 0.0, 0.7945]\n",
      "\n",
      "=========================================\n",
      "\n",
      "d1 has a weights of 0.6 0.746499254419 0.79447399635 for pnorms of 1, 10 and 100 respectively\n",
      "d3 has a weights of 0.4 0.746426393229 0.79447399635 for pnorms of 1, 10 and 100 respectively\n",
      "d5 has a weights of 0.4 0.746426393229 0.79447399635 for pnorms of 1, 10 and 100 respectively\n",
      "d2 has a weights of 0.2 0.373213196615 0.397236998175 for pnorms of 1, 10 and 100 respectively\n",
      "d4 has a weights of 0.0 0.0 0.0 for pnorms of 1, 10 and 100 respectively\n"
     ]
    }
   ],
   "source": [
    "ebm.pnorm = 1\n",
    "weights1 = OR('relevant', 'retrieval')\n",
    "ebm.pnorm = 10\n",
    "weights2 = OR('relevant', 'retrieval')\n",
    "ebm.pnorm = 100\n",
    "weights3 = OR('relevant', 'retrieval')\n",
    "\n",
    "for document, weight1, weight2, weight3 in \\\n",
    "        sorted(zip(ebm.documents, weights1, weights2, weights3), key= lambda x: x[1], reverse = True):\n",
    "    print document.title, \"has a weights of\", weight1, weight2, weight3, \"for pnorms of 1, 10 and 100 respectively\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6 0.4\n",
      "0.1 0.68299059407 0.478141237146\n",
      "0.1 0.766083355147 0.560237874087\n",
      "0.1 0.791382410555 0.589788405722\n",
      "0.1 0.797836844868 0.597430644527\n"
     ]
    }
   ],
   "source": [
    "# Checking the behaviour\n",
    "\n",
    "for i in range(5):\n",
    "    p = 4**i \n",
    "    print (0.1**(p))**(1/p), ( (0.4**p + 0.8**p)/2)**(1/p), ( (0.2**p + 0.4**p + 0.6**p)/3)**(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6 0.6\n",
      "0.1 0.68299059407 0.529150262213\n",
      "0.1 0.766083355147 0.574616601241\n",
      "0.1 0.791382410555 0.593536807916\n",
      "0.1 0.797836844868 0.598377633651\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    p = 4**i \n",
    "    print (0.1**(p))**(1/p), ( (0.4**p + 0.8**p)/2)**(1/p), ( (0.2**p + 0.4**p + 0.6**p)/2)**(1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip and Splat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]] as list of columns\n",
      "[(1, 4), (2, 5), (3, 6)] as list of rows\n",
      "[(1, 2, 3), (4, 5, 6)] back to list of columns\n"
     ]
    }
   ],
   "source": [
    "matrix = [[1,2,3],[4,5,6]]\n",
    "\n",
    "print matrix, \"as list of columns\"\n",
    "print zip (*matrix), \"as list of rows\"\n",
    "print zip(*zip (*matrix)), \"back to list of columns\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
