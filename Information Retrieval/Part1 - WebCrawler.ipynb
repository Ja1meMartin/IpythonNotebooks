{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Queue\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "visited_pages = {} # Dictionary of pages with value a list of links found on that page\n",
    "visited_pages_list = [] # Keep trck of the order in which pages were visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_source(old_absolute_link):\n",
    "    # assert that we are visiting a new page\n",
    "    try:\n",
    "        assert(old_absolute_link not in visited_pages_list)\n",
    "    except:\n",
    "        \"\"\"\n",
    "            If we have visited the page already then\n",
    "            we should act according to our reindexing strategy\n",
    "            Since the pages are static we do not need one\n",
    "        \"\"\"\n",
    "        print \"PAGE ALREADY VISITED\", old_absolute_link\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    visited_pages[old_absolute_link] = [] # We will add links found on the page to this list\n",
    "    visited_pages_list.append(old_absolute_link) # This list retains the order in which\n",
    "    \n",
    "    with open(old_absolute_link) as f:\n",
    "        source = f.read()\n",
    "        return source\n",
    "    \n",
    "seed = \"literature.offline/authors/carroll-lewis/index.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(old_absolute_link, soup):\n",
    "    \n",
    "    # only pages with these extensions are valid crawls\n",
    "    valid_pages_extensions = ['htm', 'html']\n",
    "    \n",
    "    def not_visited(href): return href not in visited_pages_list and href not in page_queue.queue\n",
    "    \n",
    "    def not_online(href):  return not (href.startswith('http:') or href.startswith('https:') or href.startswith('www.'))\n",
    "    \n",
    "    def get_link_extension(href): return href.split('.')[-1]\n",
    "    \n",
    "    def valid_crawl(href): return not_online(href) and get_link_extension(href).lower() in valid_pages_extensions\n",
    "    \n",
    "    def get_new_absolute_link(absolute, relative):\n",
    "        \"\"\"\n",
    "            absolute is a link in the format a/b/c/d.html\n",
    "            relative is a link with directions from the current directory\n",
    "            if relative is e.html then we simply replace d.html with e.html\n",
    "            giving a/b/c/e.html\n",
    "            \n",
    "            if relative starts with one or more ../ then we need to remove earlier directories\n",
    "            a/b/c/d.html + ../../e.html\n",
    "            we have to go back two directories (remove b and c)\n",
    "            giving a/e.html\n",
    "        \"\"\"\n",
    "    \n",
    "        \n",
    "        back_directories_counter = 0\n",
    "        \n",
    "        while (relative.startswith(\"../\")):\n",
    "            relative = relative[3:]\n",
    "            back_directories_counter += 1\n",
    "        \n",
    "        components = absolute.split(\"/\")[:-(1+back_directories_counter)]\n",
    "        components.append(relative)\n",
    "        new_link = \"/\".join(components).split('#')[0] # must check not seen\n",
    "        return \"/\".join(components)\n",
    "    \n",
    "    \n",
    "    this_pages_links_set = set()\n",
    "    \n",
    "    \n",
    "    for link in soup.findAll('a'):\n",
    "        \n",
    "        try:\n",
    "            href = link['href']\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        new_absolute_link = get_new_absolute_link(old_absolute_link, href)\n",
    "        \n",
    "        if valid_crawl(href) and valid_crawl(new_absolute_link):\n",
    "            # Obviously a set does not allow duplicates\n",
    "            # but we want to retain order so we have to check\n",
    "            # anyway because lists do allow duplicates\n",
    "            if new_absolute_link not in this_pages_links_set:\n",
    "                this_pages_links_set.add(new_absolute_link)\n",
    "                visited_pages[old_absolute_link].append(new_absolute_link)\n",
    "            # if not visited\n",
    "            if not_visited(new_absolute_link):\n",
    "                page_queue.put(new_absolute_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pages to visit\n",
    "page_queue = Queue.Queue()\n",
    "\n",
    "page_queue.put(seed)\n",
    "\n",
    "# While there are pages in the queue\n",
    "# Open and read them\n",
    "# Create a BeautifulSoup object and pass them to get_links to extract links\n",
    "while (page_queue.qsize() > 0):\n",
    "    next_page_link = page_queue.get()\n",
    "    source = get_source(next_page_link)\n",
    "    \n",
    "    if source: # Don't revisit old pages\n",
    "        soup = BeautifulSoup(source)\n",
    "        get_links(next_page_link, soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"Crawl/Crawl.txt\"\n",
    "\n",
    "# Prints each page and the links on that page\n",
    "with open(filename, \"a+\") as f:\n",
    "    for page in visited_pages_list:\n",
    "        f.write( page + \"\\n\" )\n",
    "        for link in visited_pages[page]:\n",
    "            f.write( \"---> \" + link + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save visited_pages_list and visited_pages for use in the other parts\n",
    "pickle.dump(visited_pages, open( \"vp.p\", \"w\" ) )\n",
    "pickle.dump(visited_pages_list, open( \"vpl.p\", \"w\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
