{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---a---\n",
      "---at---\n",
      "---are---\n",
      "---for---\n",
      "---of---\n",
      "---I---\n",
      "---is---\n",
      "---there---\n",
      "---then---\n",
      "---many---\n",
      "---do---\n",
      "---to---\n",
      "---and---\n",
      "---by---\n",
      "---the---\n",
      "---not---\n",
      "---have---\n",
      "---with---\n",
      "---while---\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"a , at, are, for, of, I , is, there, then, many, do, to, and, by, the, not, have, with, while\"\"\"\n",
    "\n",
    "stop_words = [s.strip() for s in string.split(',')]\n",
    "\n",
    "for s in stop_words:\n",
    "    print \"---\"+s+\"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Corpus into Lines and Check for Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---d1: for English model retrieval have a relevance model while vector space\n",
      "model retrieval do not---\n",
      "---d2: The R-precision measure is relevant to average precision measure.---\n",
      "---d3: The most efficient retrieval models are language model and vector space\n",
      "model.---\n",
      "---d4: The English language is the most efficient language.---\n",
      "---d5: Retrieval efficiency is measured by the average precision of the\n",
      "retrieval model.---\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "d1: for English model retrieval have a relevance model while vector space\n",
    "model retrieval do not;\n",
    "d2: The R-precision measure is relevant to average precision measure.;\n",
    "d3: The most efficient retrieval models are language model and vector space\n",
    "model.;\n",
    "d4: The English language is the most efficient language.;\n",
    "d5: Retrieval efficiency is measured by the average precision of the\n",
    "retrieval model.\n",
    "\"\"\"\n",
    "\n",
    "stems = {\n",
    "    \"models\":\"model\",\n",
    "    \"r-precision\":\"precis\",\n",
    "    \"precision\":\"precis\",\n",
    "    \"precise\":\"precis\",\n",
    "    \"efficient\":\"effic\",\n",
    "    \"efficiency\":\"effic\",\n",
    "    #\"recall\":\"retrieval\",\n",
    "    \"relevant\":\"relevan\",\n",
    "    \"relevance\":\"relevan\",\n",
    "    \"measured\":\"measure\",\n",
    "}\n",
    "\n",
    "corpus = [c.strip() for c in corpus.split(\";\") if c]\n",
    "\n",
    "# Check no blank lines\n",
    "# I often surround things with \"---\" to check for whitespace\n",
    "for c in corpus:\n",
    "    print \"---\"+c+\"---\"\n",
    "\n",
    "corpus_length = len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Document = namedtuple('Document',['title', 'keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "        stems = {\n",
    "            \"models\":\"model\",\n",
    "            \"r-precision\":\"precis\",\n",
    "            \"precision\":\"precis\",\n",
    "            \"precise\":\"precis\",\n",
    "            \"efficient\":\"effic\",\n",
    "            \"efficiency\":\"effic\",\n",
    "            #\"recall\":\"retrieval\",\n",
    "            \"relevant\":\"relevan\",\n",
    "            \"relevance\":\"relevan\",\n",
    "            \"measured\":\"measure\",\n",
    "        }\n",
    "        \n",
    "        word = remove_punctuation(word)\n",
    "        return stems.get(word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_text(sentence):\n",
    "    '''\n",
    "        Prepare words to be processed by:\n",
    "            - making lowercase\n",
    "            - splitting on space\n",
    "            - removing stop words\n",
    "            - removing punctuation from end of word\n",
    "            - remove empty strings\n",
    "            - remove unique words with set\n",
    "    '''\n",
    "    sentence = sentence.lower()\n",
    "    words = [stem(w) for w in sentence.split() if w not in stop_words]\n",
    "    \n",
    "    # remove empty words\n",
    "    # use set to remove unique words\n",
    "    return {w for w in words if w}\n",
    "    \n",
    "def remove_punctuation(word):\n",
    "    '''\n",
    "        If the last digit is not alpha then remove it\n",
    "        Assumption: no numbers\n",
    "    '''\n",
    "\n",
    "    if not word[-1].isalpha():\n",
    "        word = word[:-1]\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title d1 keywords set(['space', 'relevan', 'vector', 'english', 'model', 'retrieval'])\n",
      "title d2 keywords set(['precis', 'relevan', 'average', 'measure'])\n",
      "title d3 keywords set(['language', 'space', 'most', 'vector', 'model', 'effic', 'retrieval'])\n",
      "title d4 keywords set(['most', 'effic', 'language', 'english'])\n",
      "title d5 keywords set(['average', 'precis', 'measure', 'model', 'effic', 'retrieval'])\n",
      "['space', 'relevan', 'vector', 'english', 'model', 'retrieval', 'precis', 'average', 'measure', 'language', 'most', 'effic']\n"
     ]
    }
   ],
   "source": [
    "words_list = []\n",
    "words_set = set()\n",
    "documents = []\n",
    "\n",
    "for c in corpus:\n",
    "    doc, text = c.split(\": \")\n",
    "    keywords = process_text(text)\n",
    "    \n",
    "    current_document = Document(title = doc.strip(), keywords=keywords)\n",
    "    \n",
    "    documents.append(current_document)\n",
    "    print \"title\", current_document.title, \"keywords\", current_document.keywords\n",
    "    \n",
    "    for kw in keywords:\n",
    "        if not kw in words_set:\n",
    "            words_list.append(kw)\n",
    "            words_set.add(kw)\n",
    "            \n",
    "print words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d1: for English model retrieval have a relevance model while vector space\\nmodel retrieval do not', 'd2: The R-precision measure is relevant to average precision measure.', 'd3: The most efficient retrieval models are language model and vector space\\nmodel.', 'd4: The English language is the most efficient language.', 'd5: Retrieval efficiency is measured by the average precision of the\\nretrieval model.']\n",
      "language [0, 0, 1, 1, 0]\n",
      "space [1, 0, 1, 0, 0]\n",
      "average [0, 1, 0, 0, 1]\n",
      "measure [0, 1, 0, 0, 1]\n",
      "precis [0, 1, 0, 0, 1]\n",
      "most [0, 0, 1, 1, 0]\n",
      "relevan [1, 1, 0, 0, 0]\n",
      "vector [1, 0, 1, 0, 0]\n",
      "english [1, 0, 0, 1, 0]\n",
      "model [1, 0, 1, 0, 1]\n",
      "effic [0, 0, 1, 1, 1]\n",
      "retrieval [1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "index = {}\n",
    "\n",
    "\n",
    "for word in words_list:\n",
    "    vector = []\n",
    "    \n",
    "    for d in documents:\n",
    "        vector.append(1) if word in d.keywords else vector.append(0)\n",
    "        index[word] = vector\n",
    "        \n",
    "print corpus\n",
    "        \n",
    "    \n",
    "for key,value in index.iteritems():\n",
    "    print key,value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Index\n",
    "\n",
    "Using corpus frequency as (number of documents a term appears in)/(total number of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': [0, 0, 0.4, 0.4, 0], 'space': [0.4, 0, 0.4, 0, 0], 'average': [0, 0.4, 0, 0, 0.4], 'english': [0.4, 0, 0, 0.4, 0], 'precis': [0, 0.4, 0, 0, 0.4], 'most': [0, 0, 0.4, 0.4, 0], 'relevan': [0.4, 0.4, 0, 0, 0], 'vector': [0.4, 0, 0.4, 0, 0], 'measure': [0, 0.4, 0, 0, 0.4], 'model': [0.6, 0, 0.6, 0, 0.6], 'effic': [0, 0, 0.6, 0.6, 0.6], 'retrieval': [0.6, 0, 0.6, 0, 0.6]}\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "    \n",
    "def term_weights(l):\n",
    "    \"\"\"\n",
    "        Input is a vector of 1's and 0's indicating if a term is in a document.\n",
    "        If we get the sum of this vector we have the number of documents it appears in\n",
    "        We divide this by the length of the corpus\n",
    "    \"\"\"\n",
    "    total = sum(l)\n",
    "    return [total*1.0/corpus_length if item else 0 for item in l]\n",
    "    \n",
    "weighted_index = {key:term_weights(value) for key,value in index.iteritems()}\n",
    "print weighted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q(keyword):\n",
    "    # Search for a term in the index if not there return a vector of zeros\n",
    "    return index.get(stem(keyword), [0]*corpus_length)\n",
    "\n",
    "def AND(*args):\n",
    "    \"\"\"\n",
    "        Takes in a list of lists. The inner list are 1's and 0's representing a term in a document\n",
    "        zip(*args) combines the value at index i for each list for i = 0 to length-1\n",
    "        if a term is true at all positons return true else return false\n",
    "    \"\"\"\n",
    "    return[all(x) for x in zip(*args)]\n",
    "\n",
    "def OR(*args):\n",
    "    \"\"\"\n",
    "        Takes in a list of lists. The inner list are 1's and 0's representing a term in a document\n",
    "        zip(*args) combines the value at index i for each list for i = 0 to length-1\n",
    "        if a term is true at any positon return true else return false\n",
    "    \"\"\"\n",
    "    return[any(x) for x in zip(*args)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval or Relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant [1, 1, 0, 0, 0]\n",
      "retrieval [1, 0, 1, 0, 1]\n",
      "Answer: ['d1', 'd2', 'd3', 'd5']\n"
     ]
    }
   ],
   "source": [
    "answer = OR(q('relevant'), q('retrieval') )\n",
    "\n",
    "for term in ('relevant','retrieval'):\n",
    "    print term, q(term)\n",
    "    \n",
    "print \"Answer:\",[y[0] for x,y in zip(answer, documents) if x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient and Model and Efficient and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficient [0, 0, 1, 1, 1]\n",
      "model [1, 0, 1, 0, 1]\n",
      "efficient [0, 0, 1, 1, 1]\n",
      "retrieval [1, 0, 1, 0, 1]\n",
      "Answer: ['d3', 'd5']\n"
     ]
    }
   ],
   "source": [
    "answer = AND (q('efficient'), q('model'), q('efficient'), q('retrieval'))\n",
    "\n",
    "for term in ('efficient', 'model','efficient','retrieval'):\n",
    "    print term, q(term)\n",
    "print \"Answer:\",[y[0] for x,y in zip(answer, documents) if x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Precise and Recall) or average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precise, recall [0, 1, 0, 0, 1] [0, 0, 0, 0, 0]\n",
      "precise AND recall: [False, False, False, False, False]\n",
      "average: [0, 1, 0, 0, 1]\n",
      "Answer: ['d2', 'd5']\n"
     ]
    }
   ],
   "source": [
    "clause1 = AND(q('precise'), q('recall'))\n",
    "average = q('average')\n",
    "answer = OR(clause1, average)\n",
    "\n",
    "print \"precise, recall\", q('precise'), q('recall')\n",
    "print \"precise AND recall:\", clause1\n",
    "print \"average:\", average\n",
    "print \"Answer:\",[y[0] for x,y in zip(answer, documents) if x]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
