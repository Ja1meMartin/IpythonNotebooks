{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Stemmer\n",
    "\n",
    "Running the next cell will open a window with a spider's web type icon. You can go to that to download the stemmer under models> Punkt > Punkt Tokenizer Models  \n",
    "  \n",
    "  \n",
    "### The program will not continue until you close the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://nltk.github.com/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "This is the English stopword list from the Python Natural Language Toolit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
    "             'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "             'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',\n",
    "             'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "             'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
    "             'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "             'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "             'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `SnowballStemmer # print the function definition` not found.\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") # ignore stopwords is set to false automatically\n",
    "%pdef SnowballStemmer # print the function definition\n",
    "\n",
    "stemmer.stopwords = stopwords #stopwords.words('english')\n",
    "\n",
    "print stemmer.stem(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames = [\"Austen_Emma\",\"Austen_Pride\",\"Austen_Sense\",\"CBronte_Jane\",\"CBronte_Professor\",\"CBronte_Villette\"]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename+\".txt\", 'r') as document:\n",
    "        documents.append((filename, document.readlines())) # add the lines of the file to documents\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_end_punctuation_and_numbers(word):\n",
    "    '''\n",
    "        If the last digit is not alpha then remove it\n",
    "    '''\n",
    "\n",
    "    # Don't return empty strings\n",
    "    if not word:\n",
    "        return None\n",
    "    \n",
    "    # Working from the last character recursively remove trailing punctuation\n",
    "    if not word[-1].isalpha():\n",
    "        word = word[:-1]\n",
    "        word = remove_end_punctuation_and_numbers(word)\n",
    "    \n",
    "    # Don't return empty strings\n",
    "    if not word:\n",
    "        return None\n",
    "      \n",
    "    # Starting at the begiining recursively remove leading punctuation\n",
    "    if not word[0].isalpha():\n",
    "        word = word[1:]\n",
    "        word = remove_end_punctuation_and_numbers(word)\n",
    "    \n",
    "    #if len(word)>2 and word[-2:] == \"'s\":\n",
    "     #   return word[:-2]\"\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    if stem:\n",
    "        word = stemmer.stem(word)\n",
    "        \n",
    "    return word\n",
    "\n",
    "def clean_word(word):\n",
    "    \"\"\"\n",
    "        remove whitespace and make lower\n",
    "        pass to remove punctuation\n",
    "    \"\"\"\n",
    "    word = word.strip().lower()\n",
    "    \n",
    "    if not word:\n",
    "        return None\n",
    "    word = remove_end_punctuation_and_numbers(word)\n",
    "    \n",
    "    \n",
    "    return word\n",
    "    \n",
    "    \n",
    "def add_to_collection(word,add_to_collection, stem= False):\n",
    "    \"\"\"\n",
    "        split sentences on '--'\n",
    "    \"\"\"\n",
    "    for token in word.split(\"--\"): # Already split on spaces but need to check for this delimiter\n",
    "        word = clean_word(token)\n",
    "        if word:\n",
    "            add_to_collection(word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. counts the number of words in the entire corpus\n",
    "\n",
    "For each document -> for each sentence in that document -> for each word in that sentence (found by splitting the sentence on spaces)\n",
    "\n",
    "Pass it to add_to_collection along with the allwords.append function which will add the sanitized words to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871349\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "stem = False\n",
    "[add_to_collection(word, all_words.append) for filename, doc in documents  for sentence in doc for word in sentence.split()]\n",
    "\n",
    "print len(all_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above with stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407701\n"
     ]
    }
   ],
   "source": [
    "print len([word for word in all_words if word not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above with stopwords removed and stemming - should be the same as the last answer as we are counting all words not unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407701\n"
     ]
    }
   ],
   "source": [
    "all_words_stemmed = []\n",
    "\n",
    "stem = True\n",
    "\n",
    "for word in all_words:\n",
    "    if word not in stopwords:\n",
    "        add_to_collection(word, all_words_stemmed.append) \n",
    "print len(all_words_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160418\n",
      "121716\n",
      "119538\n",
      "187001\n",
      "88687\n",
      "193989\n"
     ]
    }
   ],
   "source": [
    "words_dictionary = {}\n",
    "\n",
    "\n",
    "\n",
    "for document in documents:\n",
    "    words = [] # new list\n",
    "    [add_to_collection(word, words.append) for sentence in document[1] for word in sentence.split()] # add words to list\n",
    "    words_dictionary[document[0]] = words # add list of words to a dictionary\n",
    "    print len(words)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CBronte_Villette', 'Austen_Sense', 'CBronte_Jane', 'Austen_Pride', 'CBronte_Professor', 'Austen_Emma']\n"
     ]
    }
   ],
   "source": [
    "no_stopwords_dictionary = {}\n",
    "stemmed_dictionary = {}\n",
    "\n",
    "print words_dictionary.keys()\n",
    "\n",
    "for document, words in words_dictionary.iteritems():\n",
    "    no_stopwords = []\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            no_stopwords.append(word)\n",
    "            stemmed_words.append(stemmer.stem(word))\n",
    "\n",
    "            \n",
    "    no_stopwords_dictionary[document] = no_stopwords\n",
    "    stemmed_dictionary[document] = stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CBronte_Villette',\n",
       " 'Austen_Sense',\n",
       " 'CBronte_Jane',\n",
       " 'Austen_Pride',\n",
       " 'CBronte_Professor',\n",
       " 'Austen_Emma']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stopwords_dictionary.keys()\n",
    "stemmed_dictionary.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. counts the top 100 most frequent words in any one document \n",
    "– what can be said about this list?  \n",
    "\n",
    "  \n",
    "The main thing I would say is that it is frequented by stopwords, as predicted by Zipf's Law, and main character names such as Elizabeth, Darci, Jane, Bingham etc.  \n",
    "  \n",
    "It does not however conform to the expected values of Zipf's laws which I hav included beside the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121716\n",
      "the 4330\n",
      "the 4330 4330.0 1.0\n",
      "to 4136 2165.0 1.91\n",
      "of 3609 1443.33333333 2.5\n",
      "and 3577 1082.5 3.3\n",
      "her 2232 866.0 2.58\n",
      "i 2064 721.666666667 2.86\n",
      "a 1948 618.571428571 3.15\n",
      "in 1866 541.25 3.45\n",
      "was 1847 481.111111111 3.84\n",
      "she 1710 433.0 3.95\n",
      "it 1637 393.636363636 4.16\n",
      "that 1577 360.833333333 4.37\n",
      "not 1426 333.076923077 4.28\n",
      "be 1414 309.285714286 4.57\n",
      "you 1356 288.666666667 4.7\n",
      "he 1338 270.625 4.94\n",
      "his 1270 254.705882353 4.99\n",
      "as 1181 240.555555556 4.91\n",
      "had 1177 227.894736842 5.16\n",
      "for 1058 216.5 4.89\n",
      "with 1052 206.19047619 5.1\n",
      "but 1002 196.818181818 5.09\n",
      "have 938 188.260869565 4.98\n",
      "is 860 180.416666667 4.77\n",
      "at 788 173.2 4.55\n",
      "mr 786 166.538461538 4.72\n",
      "him 764 160.37037037 4.76\n",
      "on 724 154.642857143 4.68\n",
      "my 713 149.310344828 4.78\n",
      "by 636 144.333333333 4.41\n",
      "elizabeth 635 139.677419355 4.55\n",
      "all 624 135.3125 4.61\n",
      "they 601 131.212121212 4.58\n",
      "so 589 127.352941176 4.62\n",
      "were 565 123.714285714 4.57\n",
      "which 539 120.277777778 4.48\n",
      "could 526 117.027027027 4.49\n",
      "been 516 113.947368421 4.53\n",
      "from 493 111.025641026 4.44\n",
      "no 489 108.25 4.52\n",
      "veri 487 105.609756098 4.61\n",
      "your 478 103.095238095 4.64\n",
      "what 478 100.697674419 4.75\n",
      "would 471 98.4090909091 4.79\n",
      "this 448 96.2222222222 4.66\n",
      "me 447 94.1304347826 4.75\n",
      "their 444 92.1276595745 4.82\n",
      "them 433 90.2083333333 4.8\n",
      "will 418 88.3673469388 4.73\n",
      "darci 417 86.6 4.82\n",
      "said 401 84.9019607843 4.72\n",
      "such 389 83.2692307692 4.67\n",
      "do 373 81.6981132075 4.57\n",
      "when 373 80.1851851852 4.65\n",
      "there 354 78.7272727273 4.5\n",
      "an 353 77.3214285714 4.57\n",
      "if 349 75.9649122807 4.59\n",
      "mrs 343 74.6551724138 4.59\n",
      "are 339 73.3898305085 4.62\n",
      "bennet 333 72.1666666667 4.61\n",
      "much 328 70.9836065574 4.62\n",
      "more 326 69.8387096774 4.67\n",
      "am 317 68.7301587302 4.61\n",
      "bingley 311 67.65625 4.6\n",
      "must 308 66.6153846154 4.62\n",
      "or 299 65.6060606061 4.56\n",
      "sister 295 64.6268656716 4.56\n",
      "jane 292 63.6764705882 4.59\n",
      "miss 287 62.7536231884 4.57\n",
      "who 284 61.8571428571 4.59\n",
      "than 282 60.985915493 4.62\n",
      "one 278 60.1388888889 4.62\n",
      "know 273 59.3150684932 4.6\n",
      "did 272 58.5135135135 4.65\n",
      "ani 269 57.7333333333 4.66\n",
      "ladi 265 56.9736842105 4.65\n",
      "other 264 56.2337662338 4.69\n",
      "we 253 55.5128205128 4.56\n",
      "should 247 54.8101265823 4.51\n",
      "think 236 54.125 4.36\n",
      "how 231 53.4567901235 4.32\n",
      "herself 228 52.8048780488 4.32\n",
      "befor 227 52.1686746988 4.35\n",
      "though 226 51.5476190476 4.38\n",
      "has 221 50.9411764706 4.34\n",
      "time 221 50.3488372093 4.39\n",
      "never 220 49.7701149425 4.42\n",
      "soon 216 49.2045454545 4.39\n",
      "see 214 48.6516853933 4.4\n",
      "well 214 48.1111111111 4.45\n",
      "can 211 47.5824175824 4.43\n",
      "some 211 47.0652173913 4.48\n",
      "say 210 46.5591397849 4.51\n",
      "make 209 46.0638297872 4.54\n",
      "now 209 45.5789473684 4.59\n",
      "onli 208 45.1041666667 4.61\n",
      "after 200 44.6391752577 4.48\n",
      "might 200 44.1836734694 4.53\n",
      "everi 198 43.7373737374 4.53\n",
      "wickham 194 43.3 4.48\n"
     ]
    }
   ],
   "source": [
    "document = documents[1] # A document\n",
    "tokens = words_dictionary[document[0]] # Get words from dictionary\n",
    "print len(tokens)\n",
    "\n",
    "\"\"\"for sentence in document[1]:\n",
    "    for word in sentence.split():\n",
    "        add_to_collection(word, tokens.append)\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from __future__ import division\n",
    "\n",
    "counter = Counter(tokens) \n",
    "most_common_term, most_common_value = counter.most_common(1)[0]\n",
    "print most_common_term, most_common_value\n",
    "values = [(key, value) for key, value in counter.most_common(100)]\n",
    "\n",
    "for index, frequency_tuple in enumerate(values):\n",
    "    expected = most_common_value/(index+1)\n",
    "    key,value = frequency_tuple\n",
    "    print key, value, expected, round(value/expected,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With stopwords removed the proper nouns jump up the leader board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56289\n",
      "mr 786\n",
      "elizabeth 635\n",
      "could 526\n",
      "veri 487\n",
      "would 471\n",
      "darci 417\n",
      "said 401\n",
      "mrs 343\n",
      "bennet 333\n",
      "much 328\n",
      "bingley 311\n",
      "must 308\n",
      "sister 295\n",
      "jane 292\n",
      "miss 287\n",
      "one 278\n",
      "know 273\n",
      "ani 269\n",
      "ladi 265\n",
      "think 236\n",
      "befor 227\n",
      "though 226\n",
      "time 221\n",
      "never 220\n",
      "soon 216\n",
      "see 214\n",
      "well 214\n",
      "say 210\n",
      "make 209\n",
      "onli 208\n",
      "might 200\n",
      "everi 198\n",
      "wickham 194\n",
      "good 194\n",
      "may 193\n",
      "littl 189\n",
      "wish 183\n",
      "collin 180\n",
      "noth 179\n",
      "friend 174\n",
      "look 172\n",
      "without 171\n",
      "lydia 171\n",
      "come 168\n",
      "feel 168\n",
      "hope 167\n",
      "day 164\n",
      "even 162\n",
      "shall 162\n",
      "go 161\n",
      "famili 159\n",
      "give 157\n",
      "great 157\n",
      "like 156\n",
      "dear 155\n",
      "happi 155\n",
      "man 150\n",
      "first 145\n",
      "manner 143\n",
      "thought 140\n",
      "believ 137\n",
      "daughter 135\n",
      "mother 135\n",
      "father 135\n",
      "two 135\n",
      "howev 134\n",
      "repli 133\n",
      "young 133\n",
      "letter 131\n",
      "ever 130\n",
      "certain 127\n",
      "marri 126\n",
      "catherin 126\n",
      "made 126\n",
      "last 125\n",
      "long 125\n",
      "walk 124\n",
      "quit 123\n",
      "us 122\n",
      "room 120\n",
      "alway 119\n",
      "mani 118\n",
      "away 117\n",
      "cannot 112\n",
      "way 111\n",
      "talk 110\n",
      "hous 110\n",
      "mean 109\n",
      "receiv 109\n",
      "love 108\n",
      "return 108\n",
      "expect 108\n",
      "seem 107\n",
      "sure 107\n",
      "attent 107\n",
      "enough 106\n",
      "answer 105\n",
      "appear 104\n",
      "take 104\n",
      "saw 103\n"
     ]
    }
   ],
   "source": [
    "document = documents[1] # A document\n",
    "tokens = no_stopwords_dictionary[document[0]] # Get words from dictionary\n",
    "print len(tokens)\n",
    "\n",
    "counter = Counter(tokens) \n",
    "values = [(key, value) for key, value in counter.most_common(100)]\n",
    "\n",
    "for index, frequency_tuple in enumerate(values):\n",
    "    key,value = frequency_tuple\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, since we are not conting unique words the stemming has very little influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56289\n",
      "mr 786\n",
      "elizabeth 635\n",
      "could 526\n",
      "veri 487\n",
      "would 471\n",
      "darci 417\n",
      "said 401\n",
      "mrs 343\n",
      "bennet 333\n",
      "much 328\n",
      "bingley 311\n",
      "must 308\n",
      "sister 295\n",
      "jane 292\n",
      "miss 287\n",
      "one 278\n",
      "know 273\n",
      "ani 269\n",
      "ladi 265\n",
      "think 236\n",
      "befor 227\n",
      "though 226\n",
      "time 221\n",
      "never 220\n",
      "soon 216\n",
      "see 214\n",
      "well 214\n",
      "say 210\n",
      "make 209\n",
      "on 208\n",
      "might 200\n",
      "everi 198\n",
      "wickham 194\n",
      "good 194\n",
      "may 193\n",
      "littl 189\n",
      "collin 184\n",
      "wish 183\n",
      "noth 179\n",
      "friend 174\n",
      "look 172\n",
      "without 171\n",
      "lydia 171\n",
      "come 168\n",
      "feel 168\n",
      "hope 167\n",
      "day 164\n",
      "even 162\n",
      "shall 162\n",
      "go 161\n",
      "famili 159\n",
      "give 157\n",
      "great 157\n",
      "like 156\n",
      "dear 155\n",
      "happi 155\n",
      "man 150\n",
      "first 145\n",
      "manner 143\n",
      "thought 140\n",
      "believ 137\n",
      "daughter 135\n",
      "mother 135\n",
      "father 135\n",
      "two 135\n",
      "howev 134\n",
      "repli 133\n",
      "young 133\n",
      "letter 131\n",
      "ever 130\n",
      "certain 127\n",
      "marri 126\n",
      "catherin 126\n",
      "made 126\n",
      "last 125\n",
      "long 125\n",
      "walk 124\n",
      "quit 123\n",
      "us 122\n",
      "room 120\n",
      "alway 119\n",
      "mani 118\n",
      "away 117\n",
      "cannot 112\n",
      "way 111\n",
      "love 110\n",
      "talk 110\n",
      "hous 110\n",
      "mean 109\n",
      "receiv 109\n",
      "affect 108\n",
      "return 108\n",
      "expect 108\n",
      "seem 107\n",
      "sure 107\n",
      "attent 107\n",
      "enough 106\n",
      "answer 105\n",
      "appear 104\n",
      "take 104\n"
     ]
    }
   ],
   "source": [
    "document = documents[1] # A document\n",
    "tokens = stemmed_dictionary[document[0]] # Get words from dictionary\n",
    "print len(tokens)\n",
    "\n",
    "counter = Counter(tokens) \n",
    "values = [(key, value) for key, value in counter.most_common(100)]\n",
    "\n",
    "for index, frequency_tuple in enumerate(values):\n",
    "    key,value = frequency_tuple\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. counts the number of unique terms in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Austen_Emma there are 4629 unique words, 4526 after removing stopword, 4420 after stemming\n",
      "In Austen_Pride there are 4048 unique words, 3944 after removing stopword, 3843 after stemming\n",
      "In Austen_Sense there are 4144 unique words, 4042 after removing stopword, 3945 after stemming\n",
      "In CBronte_Jane there are 8702 unique words, 8597 after removing stopword, 8421 after stemming\n",
      "In CBronte_Professor there are 6915 unique words, 6810 after removing stopword, 6691 after stemming\n",
      "In CBronte_Villette there are 10193 unique words, 10089 after removing stopword, 9892 after stemming\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    #unique_words = set()\n",
    "    #[add_to_collection(word, unique_words.add) for sentence in document[1] for word in sentence.split()]\n",
    "    unique_words = set(words_dictionary[document[0]])\n",
    "    unique_no_stopwords = set(no_stopwords_dictionary[document[0]])\n",
    "    unique_stemmed_words = set(stemmed_dictionary[document[0]])\n",
    "    string = \"In {0} there are {1} unique words, {2} after removing stopword, {3} after stemming\"\n",
    "    print string.format(document[0], len(unique_words), len(unique_no_stopwords), len(unique_stemmed_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d. counts the frequency of each unique term in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austen_Emma top 10\n",
      "[(u'the', 5204), ('to', 5186), (u'and', 4878), ('of', 4292), ('i', 3191), ('a', 3126), ('it', 2651), (u'her', 2510), (u'was', 2400), (u'she', 2364)]\n",
      "\n",
      "\n",
      "Austen_Emma top 10 without stopwords\n",
      "[(u'veri', 1212), ('mr', 1154), (u'emma', 865), (u'could', 837), (u'would', 820), (u'mrs', 701), (u'ani', 655), (u'miss', 614), (u'must', 571), (u'harriet', 506)]\n",
      "\n",
      "\n",
      "Austen_Emma top 10 without stopwords and stemming\n",
      "[(u'veri', 1212), ('mr', 1154), (u'emma', 865), (u'could', 837), (u'would', 820), (u'mrs', 701), (u'ani', 655), (u'miss', 614), (u'must', 571), (u'harriet', 506)]\n",
      "\n",
      "\n",
      "Austen_Pride top 10\n",
      "[(u'the', 4330), ('to', 4136), ('of', 3609), (u'and', 3577), (u'her', 2232), ('i', 2064), ('a', 1948), ('in', 1866), (u'was', 1847), (u'she', 1710)]\n",
      "\n",
      "\n",
      "Austen_Pride top 10 without stopwords\n",
      "[('mr', 786), (u'elizabeth', 635), (u'could', 526), (u'veri', 487), (u'would', 471), (u'darci', 417), (u'said', 401), (u'mrs', 343), (u'bennet', 333), (u'much', 328)]\n",
      "\n",
      "\n",
      "Austen_Pride top 10 without stopwords and stemming\n",
      "[('mr', 786), (u'elizabeth', 635), (u'could', 526), (u'veri', 487), (u'would', 471), (u'darci', 417), (u'said', 401), (u'mrs', 343), (u'bennet', 333), (u'much', 328)]\n",
      "\n",
      "\n",
      "Austen_Sense top 10\n",
      "[(u'the', 4105), ('to', 4104), ('of', 3571), (u'and', 3489), (u'her', 2561), ('a', 2067), ('i', 1997), ('in', 1948), ('it', 1890), (u'was', 1861)]\n",
      "\n",
      "\n",
      "Austen_Sense top 10 without stopwords\n",
      "[(u'elinor', 685), (u'could', 578), (u'mariann', 566), (u'mrs', 530), (u'would', 515), (u'veri', 500), (u'said', 397), (u'ani', 390), (u'everi', 377), (u'one', 332)]\n",
      "\n",
      "\n",
      "Austen_Sense top 10 without stopwords and stemming\n",
      "[(u'elinor', 685), (u'could', 578), (u'mariann', 566), (u'mrs', 530), (u'would', 515), (u'veri', 500), (u'said', 397), (u'ani', 390), (u'everi', 377), (u'one', 332)]\n",
      "\n",
      "\n",
      "CBronte_Jane top 10\n",
      "[(u'the', 7835), ('i', 7162), (u'and', 6619), ('to', 5155), ('a', 4460), ('of', 4359), (u'you', 2966), ('in', 2762), ('it', 2731), (u'was', 2525)]\n",
      "\n",
      "\n",
      "CBronte_Jane top 10 without stopwords\n",
      "[(u'would', 666), (u'one', 606), (u'said', 583), ('mr', 544), (u'could', 505), (u'look', 470), (u'like', 459), (u'veri', 377), (u'rochest', 371), (u'jane', 346)]\n",
      "\n",
      "\n",
      "CBronte_Jane top 10 without stopwords and stemming\n",
      "[(u'would', 666), (u'one', 606), (u'said', 583), ('mr', 544), (u'could', 505), (u'look', 470), (u'like', 459), (u'veri', 377), (u'rochest', 371), (u'jane', 346)]\n",
      "\n",
      "\n",
      "CBronte_Professor top 10\n",
      "[(u'the', 3831), (u'and', 2931), ('i', 2928), ('of', 2661), ('to', 2325), ('a', 2235), ('in', 1521), (u'her', 1254), ('my', 1108), (u'was', 1091)]\n",
      "\n",
      "\n",
      "CBronte_Professor top 10 without stopwords\n",
      "[(u'would', 281), (u'one', 256), (u'said', 226), (u'look', 210), (u'like', 195), (u'littl', 194), (u'could', 192), (u'veri', 190), (u'thought', 175), (u'eye', 172)]\n",
      "\n",
      "\n",
      "CBronte_Professor top 10 without stopwords and stemming\n",
      "[(u'would', 281), (u'one', 256), (u'said', 226), (u'look', 210), (u'like', 195), (u'littl', 194), (u'could', 192), (u'veri', 190), (u'thought', 175), (u'eye', 172)]\n",
      "\n",
      "\n",
      "CBronte_Villette top 10\n",
      "[(u'the', 8366), (u'and', 6347), ('i', 5889), ('of', 4842), ('to', 4746), ('a', 4676), ('in', 3097), (u'was', 2884), (u'it', 2641), ('he', 2216)]\n",
      "\n",
      "\n",
      "CBronte_Villette top 10 without stopwords\n",
      "[(u'would', 743), (u'said', 592), (u'one', 549), (u'could', 536), (u'littl', 515), (u'like', 484), (u'look', 472), (u'veri', 389), (u'seem', 370), (u'thought', 364)]\n",
      "\n",
      "\n",
      "CBronte_Villette top 10 without stopwords and stemming\n",
      "[(u'would', 743), (u'said', 592), (u'one', 549), (u'could', 536), (u'littl', 515), (u'like', 484), (u'look', 472), (u'veri', 389), (u'seem', 370), (u'thought', 364)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    unique_words = Counter(words_dictionary[document[0]])\n",
    "    unique_no_stopwords = Counter(no_stopwords_dictionary[document[0]])\n",
    "    unique_stemmed_words = Counter(stemmed_dictionary[document[0]])\n",
    "    \n",
    "    print document[0], \"top 10\"\n",
    "    print unique_words.most_common(10)\n",
    "    print \"\\n\"\n",
    "    \n",
    "    print document[0], \"top 10 without stopwords\"\n",
    "    print unique_no_stopwords.most_common(10)\n",
    "    print \"\\n\"\n",
    "                                   \n",
    "    print document[0], \"top 10 without stopwords and stemming\"\n",
    "    print unique_stemmed_words.most_common(10)\n",
    "    print \"\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
