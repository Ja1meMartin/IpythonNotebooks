{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2016-04-29 20:44:15\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = sqlContext.read.json(\"file:///home/bdm/twitter.small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_data(tweet):\n",
    "    username = tweet.user.screen_name\n",
    "    favs = tweet.favorite_count\n",
    "    rts = tweet.retweet_count\n",
    "    \n",
    "    return (username, (1.0, float(favs+rts)))\n",
    "\n",
    "def movie_data(tweet):\n",
    "    title = tweet.entities.urls[0].display_url\n",
    "    favs = tweet.favorite_count\n",
    "    rts = tweet.retweet_count\n",
    "    \n",
    "    return (title, (1.0, float(favs+rts)))\n",
    "\n",
    "userset = dataset.map(user_data)\n",
    "movieset = dataset.map(movie_data)\n",
    "\n",
    "#print type(movieset)\n",
    "#print userset.collect()[:5]\n",
    "#print movieset.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_interval = 15\n",
    "window_interval = batch_interval * 2\n",
    "sliding_interval = batch_interval\n",
    "ssc = StreamingContext(sc, batch_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveUser(time, rdd):\n",
    "    try:\n",
    "        from pyspark.sql import SQLContext, Row\n",
    "        sqlContext = SQLContext(sc)\n",
    "        \"\"\"\n",
    "            Input: (username, (ignored, engaged))\n",
    "        \"\"\"\n",
    "        df = sqlContext.createDataFrame(rdd.map(\\\n",
    "        lambda row: Row(time=time, username=row[0], engaged=row[1][1], ignored=row[1][0])))\n",
    "        \n",
    "        df.show()\n",
    "\n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=\"usercounts\", keyspace=\"assignment\")\\\n",
    "        .save(mode=\"append\")\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def saveMovie(time, rdd):\n",
    "    try:\n",
    "        from pyspark.sql import SQLContext, Row\n",
    "        sqlContext = SQLContext(sc)\n",
    "        \"\"\"\n",
    "            Input: (title, (popularity, engagement))\n",
    "        \"\"\"\n",
    "        df = sqlContext.createDataFrame(rdd.map(\\\n",
    "        lambda row: Row(time=time, title=row[0], engagement=row[1][1], popularity=row[1][0])))\n",
    "        \n",
    "        df.show()\n",
    "        \n",
    "        df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=\"moviecounts\", keyspace=\"assignment\")\\\n",
    "        .save(mode=\"append\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_movies(l):\n",
    "    \"\"\"\n",
    "        return (title, (popularity, engagement))\n",
    "        popularity is 1.0\n",
    "        engagement is the sum of retweets and favorites\n",
    "    \"\"\"\n",
    "    n = l.split(\",\")\n",
    "    return (n[3], (1.0, float(n[4]) + float( n[5]) ) )\n",
    "\n",
    "def add_users(l):\n",
    "    \"\"\"\n",
    "        return (title, (ignored, engaged))\n",
    "        ignored is all teets engaged and ignored and so returns 1\n",
    "        engaged returns 1 only if the tweet received a retweet or favorite\n",
    "        engagement is the sum of retweets and favorites\n",
    "    \"\"\"\n",
    "    n = l.split(\",\")\n",
    "    #engaged = 1.0 if float(n[4]) + float( n[5]) > 0 else 0.0\n",
    "    return (n[6], (1.0, float(n[4]) + float( n[5]) ))\n",
    "\n",
    "def add_tuple(t,u):\n",
    "    \"\"\"\n",
    "        to avoid null values one of 2 solutions may have worked\n",
    "        1. default values for when the other window does not have a matching key\n",
    "        2. using floats from the beginning as I think pyspark may commit int to\n",
    "        the schema if it sees one early\n",
    "    \"\"\"\n",
    "    a = t if t else (0.0,0.0)\n",
    "    b = u if u else (0.0,0.0)\n",
    "    \n",
    "    # calculate average of tuples\n",
    "    # could have used ((x+y)/2 for x,y in zip(t,u)) but am afraid to push Cassandra\n",
    "    return ( (a[0] + b[0])/2, (a[1] + b[1])/2 )\n",
    "\n",
    "def add_kv_tuple(t):\n",
    "    \"\"\"\n",
    "        (u'isaac_singer', ((1.0, 0.0), None))\n",
    "\n",
    "    \"\"\"\n",
    "    a = t[1][0] if t[1][0] else (0.0,0.0)\n",
    "    b = t[1][1] if t[1][1] else (0.0,0.0)\n",
    "    return (t[0], ( a[0] + b[0],  a[1] + b[1])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = ssc.textFileStream(\"hdfs:///user/bdm/speed\")\n",
    "movies = lines.map(lambda line: add_movies(line)).window(window_interval, sliding_interval)\n",
    "#movies.pprint()\n",
    "m = movies.reduceByKey( add_tuple  ).transform(lambda rdd: rdd.leftOuterJoin(movieset))\n",
    "mm = m.map(add_kv_tuple)\n",
    "\n",
    "#mm.pprint()\n",
    "\n",
    "\n",
    "users = lines.map(lambda line: add_users(line)).window(window_interval, sliding_interval)\n",
    "#users.pprint()\n",
    "u = users.reduceByKey( add_tuple ).transform(lambda rdd: rdd.leftOuterJoin(userset))\n",
    "u.pprint()\n",
    "uu = u.map(add_kv_tuple)\n",
    "uu.pprint()\n",
    "\n",
    "#m.pprint()\n",
    "#u.pprint()\n",
    "\n",
    "\n",
    "mm.foreachRDD(saveMovie)\n",
    "uu.foreachRDD(saveUser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "Shows users after the join and how this is mapped to null. Movies is the exact same.  \n",
    "  \n",
    "The data going into both tables is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2016-04-29 20:46:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-04-29 20:46:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-04-29 20:46:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-04-29 20:46:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-04-29 20:46:45\n",
      "-------------------------------------------\n",
      "(u'hsyn_inan', ((1.0, 0.0), None))\n",
      "(u'mxwout', ((1.0, 0.0), None))\n",
      "(u'roxannepena', ((1.0, 0.0), None))\n",
      "(u'_Afinso_', ((1.0, 0.0), None))\n",
      "(u'mortenbramsen', ((1.0, 0.0), None))\n",
      "(u'TheOmegaTapes', ((1.0, 0.0), None))\n",
      "(u'sebutiban', ((1.0, 0.0), None))\n",
      "(u'LaitjeL', ((1.0, 0.0), None))\n",
      "(u'StefanHulman', ((1.0, 0.0), None))\n",
      "(u'PeppermintShore', ((1.0, 0.0), None))\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-04-29 20:46:45\n",
      "-------------------------------------------\n",
      "(u'hsyn_inan', (1.0, 0.0))\n",
      "(u'mxwout', (1.0, 0.0))\n",
      "(u'roxannepena', (1.0, 0.0))\n",
      "(u'_Afinso_', (1.0, 0.0))\n",
      "(u'mortenbramsen', (1.0, 0.0))\n",
      "(u'TheOmegaTapes', (1.0, 0.0))\n",
      "(u'sebutiban', (1.0, 0.0))\n",
      "(u'LaitjeL', (1.0, 0.0))\n",
      "(u'StefanHulman', (1.0, 0.0))\n",
      "(u'PeppermintShore', (1.0, 0.0))\n",
      "...\n",
      "\n",
      "+----------+----------+--------------------+--------------------+\n",
      "|engagement|popularity|                time|               title|\n",
      "+----------+----------+--------------------+--------------------+\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       2.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       1.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       1.0|       2.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "|       0.0|       2.0|2016-04-29 20:46:...|imdb.com/title/tt...|\n",
      "+----------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------+--------------------+---------------+\n",
      "|engaged|ignored|                time|       username|\n",
      "+-------+-------+--------------------+---------------+\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|      hsyn_inan|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|         mxwout|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|    roxannepena|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|       _Afinso_|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|  mortenbramsen|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|  TheOmegaTapes|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|      sebutiban|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|        LaitjeL|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|   StefanHulman|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|PeppermintShore|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|    Irelandrory|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|lewisEcollymore|\n",
      "|    1.0|    1.0|2016-04-29 20:46:...|   Schmikey1864|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|     Aaron_Lout|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|         gu3ver|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|          cthue|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|       pjmoreno|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|  MukhtarSomali|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...| MoaiadAbdullah|\n",
      "|    0.0|    1.0|2016-04-29 20:46:...|  biserdimitrov|\n",
      "+-------+-------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cassandra \n",
    "\n",
    "CREATE KEYSPACE assignment WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n",
    "\n",
    "USE assignment;\n",
    "\n",
    "CREATE TABLE userCounts (\n",
    "time text,\n",
    "username text,\n",
    "ignored double,\n",
    "engaged double,\n",
    "primary key(time, username)\n",
    ");\n",
    "\n",
    "CREATE TABLE movieCounts (\n",
    "time text,\n",
    "title text,\n",
    "popularity double,\n",
    "engagement double,\n",
    "primary key(time, title)\n",
    ");\n",
    "\n",
    "\n",
    "Below are the movieTable, userTable and the number of rows in each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Cassandra Movies Table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Cassandra Users Table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Cassandra Rows.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real ife the feed would be fed by a live source where data would be written to the HDFS folder that the textFileStream was watching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
